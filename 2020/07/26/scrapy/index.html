<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="scrapy"><meta name="keywords" content="python"><meta name="author" content="hero576"><meta name="copyright" content="hero576"><title>scrapy | void land space</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.8.2"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5ff8fca367285c16c6dd5c16ca2ccc1b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.0.2'
} </script><meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="void land space" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text"> 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD"><span class="toc-number">1.1.</span> <span class="toc-text"> 功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">1.2.</span> <span class="toc-text"> 安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8"><span class="toc-number">1.3.</span> <span class="toc-text"> 简单使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E4%BA%92%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.4.</span> <span class="toc-text"> 交互模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E6%B1%87%E6%80%BB"><span class="toc-number">1.5.</span> <span class="toc-text"> 指令汇总</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE"><span class="toc-number">2.</span> <span class="toc-text"> 设置</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text"> 选择器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E5%99%A8"><span class="toc-number">4.</span> <span class="toc-text"> 执行器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#item%E5%92%8C%E7%AE%A1%E9%81%93"><span class="toc-number">5.</span> <span class="toc-text"> item和管道</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#item"><span class="toc-number">5.1.</span> <span class="toc-text"> item</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pipeline"><span class="toc-number">5.2.</span> <span class="toc-text"> pipeline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#imagepipeline"><span class="toc-number">5.3.</span> <span class="toc-text"> ImagePipeline</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">6.</span> <span class="toc-text"> 中间件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">6.1.</span> <span class="toc-text"> 爬虫中间件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">6.2.</span> <span class="toc-text"> 下载中间件</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#scrapy%E5%88%86%E5%B8%83%E5%BC%8F"><span class="toc-number">7.</span> <span class="toc-text"> scrapy分布式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-2"><span class="toc-number">7.1.</span> <span class="toc-text"> 安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="toc-number">7.2.</span> <span class="toc-text"> 实现增量式爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%ABredisspider"><span class="toc-number">7.3.</span> <span class="toc-text"> 分布式爬虫RedisSpider</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%ABrediscrawlspider"><span class="toc-number">7.4.</span> <span class="toc-text"> 分布式爬虫RedisCrawlSpider</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scrapy%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="toc-number">7.5.</span> <span class="toc-text"> scrapy分布式部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-3"><span class="toc-number">7.5.1.</span> <span class="toc-text"> 安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8"><span class="toc-number">7.5.2.</span> <span class="toc-text"> 配置和使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#api%E5%B0%81%E8%A3%85"><span class="toc-number">7.5.3.</span> <span class="toc-text"> api封装</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#crawlspider"><span class="toc-number">8.</span> <span class="toc-text"> crawlspider</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8"><span class="toc-number">8.1.</span> <span class="toc-text"> 使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spider"><span class="toc-number">8.2.</span> <span class="toc-text"> spider</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/images/haimianbaobao.jpg"></div><div class="author-info__name text-center">hero576</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">60</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">13</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">5</span></a></div></div></div><div id="content-outer"><div class="plain" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">void land space</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">文章</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div></div><div class="layout" id="content-inner"><article id="post"><div class="plain" id="post-title">scrapy</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-07-26</time><span class="post-meta__separator">|</span><i class="fa fa-inbox" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/programme/"> programme</a></div><div class="article-container" id="post-content"><blockquote>
<p>scrapy记录</p>
</blockquote>
<a id="more"></a>
<h1 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h1>
<h2 id="功能"><a class="markdownIt-Anchor" href="#功能"></a> 功能</h2>
<ul>
<li>持久化</li>
<li>数据下载</li>
<li>数据解析</li>
<li>分布式</li>
</ul>
<h2 id="安装"><a class="markdownIt-Anchor" href="#安装"></a> 安装</h2>
<table>
<thead>
<tr>
<th>安装</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>执行安装命令</td>
<td>linux: <code>pip3 install scrapy</code><br>windows <code>python -m pip install -U pip</code><br><code>pip intsall lxml</code><br><code>pip install wheel</code><br><code>wget https://download.lfd.uci.edu/pythonlibs/w3jqiv8s/Twisted-20.3.0-cp37-cp37m-win_amd64.whl</code><br><code>pip install Twisted-20.3.0-cp37-cp37m-win_amd64.whl</code><br><code>pip install pywin32</code><br><code>pip install scrapy</code><br></td>
</tr>
<tr>
<td>检测是否安装成功</td>
<td>终端执行<code>scrapy</code></td>
</tr>
</tbody>
</table>
<ul>
<li>遇到</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">from cryptography.hazmat.bindings._openssl import ffi, lib</span><br><span class="line">ImportError: DLL load failed: 找不到指定的程序。</span><br></pre></td></tr></table></figure>
<ul>
<li>需要重新安装openssl</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip uninstall pyopenssl</span><br><span class="line">pip uninstall cryptography</span><br><span class="line">pip install pyopenssl</span><br><span class="line">pip install cryptography</span><br></pre></td></tr></table></figure>
<h2 id="简单使用"><a class="markdownIt-Anchor" href="#简单使用"></a> 简单使用</h2>
<table>
<thead>
<tr>
<th>简单使用</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>创建项目</td>
<td><code>scrapy startproject 项目名称</code></td>
</tr>
<tr>
<td>创建任务</td>
<td><code>cd 项目名称</code><br><code>scrapy genspider 任务名称 www.baidu.com</code></td>
</tr>
<tr>
<td>执行任务</td>
<td><code>scrapy crawl 任务名称</code></td>
</tr>
<tr>
<td>pycharm调试</td>
<td><code>from scrapy import cmdline``cmdline.execute(&quot;scrapy crawl dmoz&quot;.split())</code></td>
</tr>
</tbody>
</table>
<h2 id="交互模式"><a class="markdownIt-Anchor" href="#交互模式"></a> 交互模式</h2>
<ul>
<li>进入交互模式：<code>scrapy shell quotes.toscrap.com</code></li>
<li>执行命令：<code>quotes = response.css('.quote')</code></li>
</ul>
<h2 id="指令汇总"><a class="markdownIt-Anchor" href="#指令汇总"></a> 指令汇总</h2>
<table>
<thead>
<tr>
<th>global指令</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>startproject</td>
<td>创建项目</td>
</tr>
<tr>
<td>genspider</td>
<td>创建spider</td>
</tr>
<tr>
<td>settings</td>
<td>获取配置信息</td>
</tr>
<tr>
<td>runspider</td>
<td>执行spider</td>
</tr>
<tr>
<td>shell</td>
<td>命令行交互模式</td>
</tr>
<tr>
<td>fetch</td>
<td>简单抓取某个网页</td>
</tr>
<tr>
<td>view</td>
<td>把网页保存成文件，再用浏览器打开</td>
</tr>
<tr>
<td>version</td>
<td>scrapy版本信息</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>项目指令</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>crawl</td>
<td>执行spider的方法</td>
</tr>
<tr>
<td>check</td>
<td>检查代码是否有错误</td>
</tr>
<tr>
<td>list</td>
<td>所有spider名称</td>
</tr>
<tr>
<td>edit</td>
<td>编辑spider</td>
</tr>
<tr>
<td>parse</td>
<td></td>
</tr>
<tr>
<td>bench</td>
<td>测试，运行的性能</td>
</tr>
</tbody>
</table>
<h1 id="设置"><a class="markdownIt-Anchor" href="#设置"></a> 设置</h1>
<ul>
<li>设置settings.py文件</li>
</ul>
<table>
<thead>
<tr>
<th>设置示例</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>设置log级别</td>
<td><code>LOG_LEVEL = &quot;WARNING” # DEBUG INFO WARN ERROR FATAL</code></td>
</tr>
<tr>
<td>项目名称</td>
<td><code>BOT_NAME=默认： 'scrapybot' 这将用于默认情况下构造 User-Agent，也用于日志记录。当您使用startproject命令创建项目时，它会自动填充您的项目名称。</code></td>
</tr>
<tr>
<td>启用中间件</td>
<td>设置请求头	<code>USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'</code></td>
</tr>
<tr>
<td>是否遵守robots协议，默认是遵守</td>
<td><code>ROBOTSTXT_OBEY = True</code></td>
</tr>
<tr>
<td>设置并发请求的数量，默认是16个</td>
<td><code>CONCURRENT_REQUESTS</code></td>
</tr>
<tr>
<td>下载延迟，默认无延迟</td>
<td><code>DOWNLOAD_DELAY</code></td>
</tr>
<tr>
<td>开启cookie，每次请求带上前一次cookie，默认开启</td>
<td><code>COOKIES_ENABLED</code></td>
</tr>
<tr>
<td>开启cookie显示</td>
<td><code>COOKIES_DEBUG</code></td>
</tr>
<tr>
<td>设置默认请求头</td>
<td><code>DEFAULT_REQUEST_HEADERS</code></td>
</tr>
<tr>
<td>爬虫中间件，设置过程和管道相同</td>
<td><code>SPIDER_MIDDLEWARES</code></td>
</tr>
<tr>
<td>下载中间件</td>
<td><code>DOWNLOADER_MIDDLEWARES</code></td>
</tr>
</tbody>
</table>
<h1 id="选择器"><a class="markdownIt-Anchor" href="#选择器"></a> 选择器</h1>
<ul>
<li>response.selector类</li>
</ul>
<table>
<thead>
<tr>
<th>选择器</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>xpath</td>
<td>response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法</td>
<td><code>response.selector.xpath(&quot;//title/text()&quot;)</code></td>
</tr>
<tr>
<td>css</td>
<td></td>
<td><code>response.selector.css(&quot;title::text&quot;)</code></td>
</tr>
<tr>
<td>简写模式</td>
<td></td>
<td><code>response.xpath(&quot;//title/text()&quot;).extract_first()</code><br><code>response.css(&quot;title::text&quot;).extract_first()</code></td>
</tr>
<tr>
<td>迭代查找</td>
<td></td>
<td><code>response.xpath(&quot;//title/text()&quot;).css(&quot;title::text&quot;)</code></td>
</tr>
<tr>
<td>获取属性</td>
<td></td>
<td><code>response.xpath(&quot;//title&quot;).css(&quot;img::attr(src)&quot;)</code></td>
</tr>
<tr>
<td>防止报错，设置默认提取</td>
<td></td>
<td><code>response.xpath(&quot;//title/text()&quot;).extract_first(default='')</code></td>
</tr>
<tr>
<td>获取内容</td>
<td>extract() 返回一个包含有字符串的列表<br>extract_first() 返回列表中的第一个字符串，列表为空没有返回None</td>
<td>``</td>
</tr>
<tr>
<td>正则表达式匹配</td>
<td></td>
<td><code>response.re(&quot;name:'(.*?)'&quot;)</code><br><code>response.re_first(&quot;name:'(.*?)'&quot;)</code></td>
</tr>
</tbody>
</table>
<h1 id="执行器"><a class="markdownIt-Anchor" href="#执行器"></a> 执行器</h1>
<ul>
<li>scrapy.spider.Spider基类</li>
</ul>
<table>
<thead>
<tr>
<th>执行器</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>spider名字，是任务的唯一标识</td>
<td></td>
</tr>
<tr>
<td>start_urls</td>
<td>URL列表，起始。要实现不停的爬取，需要重写start_url，设置while、True，然后将don’t_filter设置为True</td>
<td></td>
</tr>
<tr>
<td>allowed_domains</td>
<td>需要开启offsiteMiddleware，允许爬取的域名列表。需要抓取的、url地址必须属于allowed_domains,但是start_urls中的url地址没有这个限制</td>
<td></td>
</tr>
<tr>
<td>crawler</td>
<td>提供对spider核心组件的访问</td>
<td></td>
</tr>
<tr>
<td>setting</td>
<td>控制核心插件的配置</td>
<td></td>
</tr>
<tr>
<td>from_crawler</td>
<td>可以通过它获取全局配置</td>
<td></td>
</tr>
<tr>
<td>custom_settings</td>
<td>覆盖settings全局的配置</td>
<td></td>
</tr>
<tr>
<td>start_requests</td>
<td>可以改写请求方式<code>Request(url=&quot;http://www.example.com&quot;,|cookies=&#123;'currency': 'USD', 'country': 'UY'&#125;)</code></td>
<td></td>
</tr>
<tr>
<td>parse</td>
<td>回调函数，spider中的parse方法必须有返回值，需要通过yield返回可以传递给管道，只能返回request、baseitem、dict和none值<br>callback：默认是parse处理的<br>headers：<br>meta：实现数据的传递，需要注意，传递字典和列表时，要用eepcopy操作，否则在后续调用时引用相同，会造成数据错乱。<br>error_back：设置错误处理的函数，决定执行的逻辑。<br>filter：请求过的url将被过滤，默认为过滤</td>
<td></td>
</tr>
<tr>
<td>log</td>
<td>message<br>level</td>
<td></td>
</tr>
<tr>
<td>closed</td>
<td></td>
<td></td>
</tr>
<tr>
<td>自定义参数</td>
<td></td>
<td></td>
</tr>
<tr>
<td>POST请求formRequest</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>response对象</li>
</ul>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>response.body</td>
<td>响应体，也就是html代码，默认是byte类型</td>
<td></td>
</tr>
<tr>
<td>response.body_as_unicode</td>
<td>响应体</td>
<td></td>
</tr>
<tr>
<td>response.copy</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.css</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.encoding</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.flags</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.follow</td>
<td>补全地址</td>
<td></td>
</tr>
<tr>
<td>response.headers</td>
<td>响应头</td>
<td></td>
</tr>
<tr>
<td>response.meta</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.replace</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.requests.headers</td>
<td>当前响应的请求头</td>
<td></td>
</tr>
<tr>
<td>response.request.url</td>
<td>当前响应对应的请求的url地址</td>
<td></td>
</tr>
<tr>
<td>response.selector</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.status</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.text</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.url</td>
<td>当前响应的url地址</td>
<td></td>
</tr>
<tr>
<td>response.urljoin</td>
<td></td>
<td></td>
</tr>
<tr>
<td>response.xpath</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>spider对象</li>
</ul>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>spider.close</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.crawler</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.custom_settings</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.from_crawler</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.handles_request</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.log</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.logger</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.make_requests_from_url</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="http://spider.name">spider.name</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.parse</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.set_crawler</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.settings</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.start_requests</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.start_urls</td>
<td></td>
<td></td>
</tr>
<tr>
<td>spider.update_settings</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>request对象
<ul>
<li><code>yield scrapy.Request(url,callback,meta)</code></li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>request.body</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.callback</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.cookies</td>
<td>设定cookie信息</td>
<td><code>scrapy.Request(cookies=cookie)</code></td>
</tr>
<tr>
<td>request.copy</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.dont_filter</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.encoding</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.errback</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.flags</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.headers</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.meta</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.method</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.priority</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.replace</td>
<td></td>
<td></td>
</tr>
<tr>
<td>request.url</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<ul>
<li>FormRequest对象</li>
<li>用于post请求<code>scrapy.FormRequest(url=url,formdata=data,callback=self.parse)</code></li>
</ul>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>formdata</td>
<td>post数据</td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="item和管道"><a class="markdownIt-Anchor" href="#item和管道"></a> item和管道</h1>
<h2 id="item"><a class="markdownIt-Anchor" href="#item"></a> item</h2>
<ul>
<li>item类型对象
<ul>
<li>用于定义要存储的数据结构</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XXXItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>
<ul>
<li>在spider获取的数据建立XXXItem对象，并对author和content赋值，yield即可传递给管道。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">item = XXXItem()</span><br><span class="line">item[<span class="string">&#x27;author&#x27;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line">item[<span class="string">&#x27;content&#x27;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line"><span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<h2 id="pipeline"><a class="markdownIt-Anchor" href="#pipeline"></a> pipeline</h2>
<ul>
<li>pipeline对象
<ul>
<li>pipeline中process_item的方法必须有，否则item没有办法接受和处理；</li>
<li>有多个pipeline的时候，process_item的方法必须return item,否则后一个pipeline取到的数据为None值</li>
<li>process_item方法接受item和spider，其中spider表示当前传递item过来的spider</li>
<li>使用之前需要在settings中开启，优先级：0~1000</li>
<li>pipeline在settings中能够开启多个，实现不同的pipeline可以处理不同爬虫的数据，不同的pipeline能够进行不同的数据处理的操作，比如一个进行数据清洗，一个进行数据的保存</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>pipeline</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>process_item</td>
<td>返回两种：1、item；2、dropitem</td>
</tr>
<tr>
<td>open_spider</td>
<td>spider开启调用一次</td>
</tr>
<tr>
<td>close_spider</td>
<td>spider关闭调用一次</td>
</tr>
<tr>
<td>from_crawler</td>
<td>获取项目配置信息</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XXXPipline</span>:</span></span><br><span class="line">    fp = <span class="literal">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">        self.fp = open(<span class="string">&quot;./xxx.txt&quot;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self,item,spider</span>):</span></span><br><span class="line">        author = item[<span class="string">&quot;author&quot;</span>]</span><br><span class="line">        content = item[<span class="string">&quot;content&quot;</span>]</span><br><span class="line">        self.fp.write(<span class="string">&quot;&#123;&#125;:&#123;&#125;\n&quot;</span>.format(author,content))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">        self.fp.close()</span><br></pre></td></tr></table></figure>
<h2 id="imagepipeline"><a class="markdownIt-Anchor" href="#imagepipeline"></a> ImagePipeline</h2>
<ul>
<li>在settings.py中设置文件保存的目录</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">IMAGES_STORE = <span class="string">&#x27;datas&#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>设置图片pipeline，还有保存的名字</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipeline <span class="keyword">import</span> ImagesPipline</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XXXImagePipline</span>(<span class="params">ImagesPipeline</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self,item,info</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [Request(x,meta=&#123;<span class="string">&#x27;item&#x27;</span>:item&#125;) <span class="keyword">for</span> x <span class="keyword">in</span> item[<span class="string">&#x27;image_url&#x27;</span>]]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">file_path</span>(<span class="params">self,request,response=None,info=None</span>):</span></span><br><span class="line">        item = request.meta[<span class="string">&quot;item&quot;</span>]</span><br><span class="line">        path = item[<span class="string">&#x27;title&#x27;</span>]item[<span class="string">&#x27;image_urls&#x27;</span>].index(request.url)+<span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> path</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span>(<span class="params">self,results,item,info</span>):</span></span><br><span class="line">      <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>ImagesPipeline</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>get_media_requests</td>
<td>根据图片地址，进行图片数据请求</td>
</tr>
<tr>
<td>file_path</td>
<td>指定图片存储的路径</td>
</tr>
<tr>
<td>item_completed</td>
<td>返回item，给下一个pipeline</td>
</tr>
</tbody>
</table>
<h1 id="中间件"><a class="markdownIt-Anchor" href="#中间件"></a> 中间件</h1>
<h2 id="爬虫中间件"><a class="markdownIt-Anchor" href="#爬虫中间件"></a> 爬虫中间件</h2>
<ul>
<li>spiderMiddleware</li>
</ul>
<h2 id="下载中间件"><a class="markdownIt-Anchor" href="#下载中间件"></a> 下载中间件</h2>
<ul>
<li>位于引擎和下载器之间，下载中间件可以拦截所有请求，以及所有请求返回相应对象。</li>
<li>功能：UA修改，Proxy修改；篡改相应</li>
</ul>
<table>
<thead>
<tr>
<th>ImagesPipeline</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>from_crawler</td>
<td></td>
</tr>
<tr>
<td>process_request</td>
<td>拦截请求<br>输入request,spider<br>需要返回数据<br>None：返回none不会对框架产生影响，继续向后执行中间件，<br>Response：返回Response，不在调用其他中间件的request方法，而调用process_response方法<br>request：返回request，则会把request重新放入项目队列中去，循环往复的调度<br>raise</td>
</tr>
<tr>
<td>process_response</td>
<td>拦截相应<br>输入：request、response、spider<br>返回<br>Response：返回Response，不会对框架产生影响，继续向后执行中间件<br>request：返回request，则会把request重新放入项目队列中去，循环往复的调度<br>raise</td>
</tr>
<tr>
<td>process_exception</td>
<td>拦截异常<br>输入：request、response、spider<br>返回：<br>None：返回None不会对框架产生影响，继续向后执行中间件<br>Response：返回Response，不在调用其他中间件的request方法，而调用process_response方法，成功的放回<br>request：返回request，则会把request重新放入项目队列中去，循环往复的调度，相当于重新发送请求<br>raise</td>
</tr>
</tbody>
</table>
<h1 id="scrapy分布式"><a class="markdownIt-Anchor" href="#scrapy分布式"></a> scrapy分布式</h1>
<ul>
<li>
<p>项目路径：<a target="_blank" rel="noopener" href="https://gitHub.com/rolando/srcapy-redis">gitHub.com/rolando/srcapy-redis</a></p>
</li>
<li>
<p>scrapy-redis是scrapy的组件，通过redis与调度器连接，存放调度器传递过来的指纹，存放request对象，默认存放redispipe保存的数据。</p>
</li>
<li>
<p>增量式爬虫，基于request对象的增量、基于数据的增量</p>
</li>
<li>
<p>实现request对象的指纹去重，使用sha1，加密请求方法、url地址、请求体得到16进制字符串。指纹存在redisset中，判断redis.sadd()存在则返回0</p>
</li>
</ul>
<h2 id="安装-2"><a class="markdownIt-Anchor" href="#安装-2"></a> 安装</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install scrapy_redis</span><br></pre></td></tr></table></figure>
<h2 id="实现增量式爬虫"><a class="markdownIt-Anchor" href="#实现增量式爬虫"></a> 实现增量式爬虫</h2>
<table>
<thead>
<tr>
<th>增量爬虫设置</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>更改调度器</td>
<td><code>SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</code></td>
</tr>
<tr>
<td>去重</td>
<td><code>DUPEFILTER_CLASS=&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</code></td>
</tr>
<tr>
<td>调度器持久化存储</td>
<td><code>SCHEDULER_PERSIST = True</code>不清空redis指纹，建议为false</td>
</tr>
<tr>
<td>redis连接</td>
<td><code>REDIS_URL=&quot;redis://root:user@ip:port&quot;</code></td>
</tr>
<tr>
<td>清空之前的所有指纹，重新爬取</td>
<td><code>SCHEDULER_FLUSH_ON_START=True</code></td>
</tr>
</tbody>
</table>
<ul>
<li>settings代码如下</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span></span><br><span class="line">SCHEDULER = <span class="string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span></span><br><span class="line">SCHEDULER_PERSIST = <span class="literal">True</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;example.pipelines.ExamplePipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">    <span class="string">&#x27;scrapy_redis.pipelines.RedisPipeline&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line">REDIS_URL = <span class="string">&quot;redis://127.0.0.1:6379&quot;</span></span><br><span class="line"> <span class="comment">#或者使用下面的方式</span></span><br><span class="line"> <span class="comment"># REDIS_HOST = &quot;127.0.0.1&quot;</span></span><br><span class="line"> <span class="comment"># REDIS_PORT = 6379</span></span><br></pre></td></tr></table></figure>
<ul>
<li>scrapy-redis将下面三个内容在redis中储存：
<ol>
<li>Scheduler队列：存放待请求的request对象，过程是pop操作，获取一个去除一个。</li>
<li>指纹集合：存放已经进入Scheduler队列的request对象的指纹，指纹默认由请求方法、url和请求体组成</li>
<li>item信息：只有开启RedisPipeline才会存入。</li>
</ol>
</li>
</ul>
<h2 id="分布式爬虫redisspider"><a class="markdownIt-Anchor" href="#分布式爬虫redisspider"></a> 分布式爬虫RedisSpider</h2>
<ul>
<li>配置跟增量相同</li>
<li>实现代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy_redis.spiders <span class="keyword">import</span> RedisSpider</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span>(<span class="params">RedisSpider</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Spider that reads urls from redis queue (myspider:start_urls).&quot;&quot;&quot;</span></span><br><span class="line">    name = <span class="string">&#x27;myspider_redis&#x27;</span></span><br><span class="line">redis_key = <span class="string">&#x27;myspider:start_urls&#x27;</span></span><br><span class="line"><span class="comment"># 手动指定allow_domains=[‘baidu.com’]</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="comment"># Dynamically define the allowed domains list.</span></span><br><span class="line">        domain = kwargs.pop(<span class="string">&#x27;domain&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        self.allowed_domains = filter(<span class="literal">None</span>, domain.split(<span class="string">&#x27;,&#x27;</span>))</span><br><span class="line">        super(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: response.css(<span class="string">&#x27;title::text&#x27;</span>).extract_first(),</span><br><span class="line">            <span class="string">&#x27;url&#x27;</span>: response.url,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>增加了一个redis_key的键，没有start_urls，因为分布式中，如果每台电脑都请求一次start_url就会重复</li>
<li>多了__init__方法，该方法不是必须的，可以手动指定allow_domains</li>
</ul>
<h2 id="分布式爬虫rediscrawlspider"><a class="markdownIt-Anchor" href="#分布式爬虫rediscrawlspider"></a> 分布式爬虫RedisCrawlSpider</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy_redis.spiders <span class="keyword">import</span> RedisCrawlSpider</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyCrawler</span>(<span class="params">RedisCrawlSpider</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Spider that reads urls from redis queue (myspider:start_urls).&quot;&quot;&quot;</span></span><br><span class="line">    name = <span class="string">&#x27;mycrawler_redis&#x27;</span></span><br><span class="line">    redis_key = <span class="string">&#x27;mycrawler:start_urls&#x27;</span></span><br><span class="line">    rules = (</span><br><span class="line">        <span class="comment"># follow all links</span></span><br><span class="line">        Rule(LinkExtractor(), callback=<span class="string">&#x27;parse_page&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="comment"># Dynamically define the allowed domains list.</span></span><br><span class="line">        domain = kwargs.pop(<span class="string">&#x27;domain&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        self.allowed_domains = filter(<span class="literal">None</span>, domain.split(<span class="string">&#x27;,&#x27;</span>))</span><br><span class="line">        super(MyCrawler, self).__init__(*args, **kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_page</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: response.css(<span class="string">&#x27;title::text&#x27;</span>).extract_first(),</span><br><span class="line">            <span class="string">&#x27;url&#x27;</span>: response.url,</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h2 id="scrapy分布式部署"><a class="markdownIt-Anchor" href="#scrapy分布式部署"></a> scrapy分布式部署</h2>
<ul>
<li>项目路径：<a target="_blank" rel="noopener" href="https://GitHub.com/scrapy/srcapyd">gitHub.com/scrapy/srcapyd</a></li>
</ul>
<h3 id="安装-3"><a class="markdownIt-Anchor" href="#安装-3"></a> 安装</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip3 install scrapyd==1.2.0a1</span><br><span class="line">pip3 install scrapyd_client</span><br></pre></td></tr></table></figure>
<h3 id="配置和使用"><a class="markdownIt-Anchor" href="#配置和使用"></a> 配置和使用</h3>
<table>
<thead>
<tr>
<th>分布式部署</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>配置远程主机</td>
<td><code>---scrapy.cfg----</code><br><code>[deploy]</code><br><code>url = http://47.95.229.68:6800/addversion.json</code><br><code>project = taobao</code></td>
</tr>
<tr>
<td>执行部署</td>
<td>scrapyd-deploy</td>
</tr>
<tr>
<td>远程调度API</td>
<td>访问<code>http://scrapyd.readthedocs.io/en/lasted/api.html</code></td>
</tr>
<tr>
<td>查看所有上传项目状态</td>
<td><code>curl http://47.95.229.68:6800/listprojects.json</code></td>
</tr>
<tr>
<td>项目版本</td>
<td><code>curl http://47.95.229.68:6800/listversions.json?project=taobao</code></td>
</tr>
<tr>
<td>远程任务启动</td>
<td><code>curl http://47.95.229.68:6800/schedule.json -d project=taobao -d spider=goods</code>请求多次可以开启多个进程</td>
</tr>
<tr>
<td>查看任务状态</td>
<td><code>curl http://47.95.229.68:6800/listjobs.json?project=taobao</code>  #可以查看job代号</td>
</tr>
<tr>
<td>任务取消</td>
<td><code>curl http://47.95.229.68:6800/cancel.json -d project=taobao -d job=job代号</code></td>
</tr>
</tbody>
</table>
<h3 id="api封装"><a class="markdownIt-Anchor" href="#api封装"></a> api封装</h3>
<ul>
<li>
<p>对scrapyd的封装，在python中就可以调度项目</p>
</li>
<li>
<p>项目地址：<a target="_blank" rel="noopener" href="https://GitHub.com/djm/python-srcapyd-api">GitHub.com/djm/python-srcapyd-api</a></p>
</li>
<li>
<p>安装：<code>pip3 install python-srcapyd-api</code></p>
</li>
</ul>
<h1 id="crawlspider"><a class="markdownIt-Anchor" href="#crawlspider"></a> crawlspider</h1>
<ul>
<li>crawlspider是scrapy的另一种模板，可以自动从response中提取所有的满足规则的url地址，自动的构造自己requests请求，发送给引擎。</li>
</ul>
<h2 id="使用"><a class="markdownIt-Anchor" href="#使用"></a> 使用</h2>
<table>
<thead>
<tr>
<th>使用</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>创建任务</td>
<td><code>scrapy genspider –t crawl 任务名 www.xxx.cn</code></td>
</tr>
</tbody>
</table>
<ul>
<li>注意点
<ol>
<li>crawlspider中不能再有以parse为名字的方法，这个方法被用来实现基础url提取功能</li>
<li>一个Rule对象接收很多参数，包含LinkExtractor、callback、和follow</li>
<li>不指定callback，如果follow为true，满足该rule的url还会继续请求</li>
<li>如果多个rule都满足某一个url，则会从rules中选择第一个进行操作</li>
</ol>
</li>
</ul>
<h2 id="spider"><a class="markdownIt-Anchor" href="#spider"></a> spider</h2>
<table>
<thead>
<tr>
<th>spider</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>rules</td>
<td>是一个元组或者是列表，包含的是Rule对象</td>
</tr>
<tr>
<td>Rule</td>
<td>表示规则，其中包含LinkExtractor,callback和follow</td>
</tr>
<tr>
<td>LinkExtractor</td>
<td>连接提取器，可以通过正则或者是xpath来进行url地址的匹配</td>
</tr>
<tr>
<td>callback</td>
<td>表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理（有需要数据提取时设置True）</td>
</tr>
<tr>
<td>follow</td>
<td>表示进过连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，True表示会，Flase表示不会（有需要url提取时设置True）</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">xxxxSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;xxxx.cn&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://xxxx.cn/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;Items/&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">      Rule(LinkExtractor(restrict_xpath=<span class="string">&#x27;//div/li&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )       <span class="comment"># 注意：正则匹配的?是有特殊用途的，所以url中?要/?处理</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        i = &#123;&#125;</span><br><span class="line">        <span class="comment">#使用xpath进行数据的提取或者url地址的提取</span></span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>LinkExtractor</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>allow(re)</td>
<td>满足正则表达式的url会被提取，如果为空，则全部匹配</td>
</tr>
<tr>
<td>deny(re)</td>
<td>满足正则表达式，则一定不提取，优先级高于allow</td>
</tr>
<tr>
<td>allow_domains(domain)</td>
<td>会被提取的domains</td>
</tr>
<tr>
<td>deny_domains(domain)</td>
<td>不会被提取的domains</td>
</tr>
<tr>
<td>restrict_xpaths</td>
<td>使用xpath表达式，和allow共同作用过滤链接</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Rule</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>link_extractor</td>
<td>linkextract对象用于定义需要提取的方法</td>
</tr>
<tr>
<td>callback</td>
<td>获得连接后，指定回调函数</td>
</tr>
<tr>
<td>follow(bool)</td>
<td>指定改规则的response是否需要跟进</td>
</tr>
<tr>
<td>process_links</td>
<td>指定该spider中那个函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数，该方法主要用来过滤url</td>
</tr>
<tr>
<td>process_request</td>
<td>指定该spider中哪个函数将会被调用，改规则提取到每个request时都会调用该函数，用于过滤request</td>
</tr>
</tbody>
</table>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text/javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.css"></div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/07/27/%E6%B1%87%E7%BC%96/"><i class="fa fa-chevron-left">  </i><span>汇编</span></a></div><div class="next-post pull-right"><a href="/2020/07/21/goproxy/"><span>goproxy</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == 'true';
var verify = 'false' == 'true';
var record_ip = '' == 'true';
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  recordIP:record_ip,
  appId:'AdfkiqY89QUSUWbDY9xJuCh0-gzGzoHsz',
  appKey:'2cEvHcqEWsyoKwy4AUL3kPGh',
  placeholder:'劈个叉吧',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'100',
  lang: 'zh-cn'
})</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2020 By hero576</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.2"></script><script src="/js/fancybox.js?version=1.8.2"></script><script src="/js/sidebar.js?version=1.8.2"></script><script src="/js/copy.js?version=1.8.2"></script><script src="/js/fireworks.js?version=1.8.2"></script><script src="/js/transition.js?version=1.8.2"></script><script src="/js/scroll.js?version=1.8.2"></script><script src="/js/head.js?version=1.8.2"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>