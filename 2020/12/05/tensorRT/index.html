<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="TensorRT"><meta name="keywords" content="algorithm"><meta name="author" content="hero576"><meta name="copyright" content="hero576"><title>TensorRT | void land space</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.8.2"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?5ff8fca367285c16c6dd5c16ca2ccc1b";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '5.0.2'
} </script><meta name="generator" content="Hexo 5.0.2"><link rel="alternate" href="/atom.xml" title="void land space" type="application/atom+xml">
</head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text"> 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text"> 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E7%9A%84%E5%B1%82"><span class="toc-number">1.2.</span> <span class="toc-text"> 支持的层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F"><span class="toc-number">1.3.</span> <span class="toc-text"> 优化方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">1.4.</span> <span class="toc-text"> 安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#linux"><span class="toc-number">1.4.1.</span> <span class="toc-text"> linux</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85pycuda"><span class="toc-number">1.5.</span> <span class="toc-text"> 安装pycuda</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85opencv"><span class="toc-number">1.6.</span> <span class="toc-text"> 安装opencv</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#windows"><span class="toc-number">1.6.1.</span> <span class="toc-text"> windows</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text"> 功能介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B"><span class="toc-number">2.1.</span> <span class="toc-text"> 流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81"><span class="toc-number">2.2.</span> <span class="toc-text"> 工作流</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8"><span class="toc-number">3.</span> <span class="toc-text"> 使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E7%BD%91%E7%BB%9C"><span class="toc-number">3.1.</span> <span class="toc-text"> 创建网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8Bapi%E6%90%AD%E5%BB%BA"><span class="toc-number">3.1.1.</span> <span class="toc-text"> 原始api搭建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E5%B7%B2%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA"><span class="toc-number">3.1.2.</span> <span class="toc-text"> 通过已生成网络搭建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAengine"><span class="toc-number">3.2.</span> <span class="toc-text"> 创建engine</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.3.</span> <span class="toc-text"> 序列化模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.4.</span> <span class="toc-text"> 执行模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#int8%E5%8A%A0%E9%80%9F"><span class="toc-number">4.</span> <span class="toc-text"> INT8加速</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B-2"><span class="toc-number">4.1.</span> <span class="toc-text"> 流程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">5.</span> <span class="toc-text"> 示例</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#gpu%E7%BC%96%E7%A8%8B"><span class="toc-number">6.</span> <span class="toc-text"> GPU编程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#c"><span class="toc-number">6.1.</span> <span class="toc-text"> C</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cpp"><span class="toc-number">6.2.</span> <span class="toc-text"> CPP</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5"><span class="toc-number">7.</span> <span class="toc-text"> 实践</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#yolov5"><span class="toc-number">7.1.</span> <span class="toc-text"> yolov5</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/images/haimianbaobao.jpg"></div><div class="author-info__name text-center">hero576</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">84</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">17</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">10</span></a></div></div></div><div id="content-outer"><div class="plain" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">void land space</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">文章</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/gallery">相册</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div></div><div class="layout" id="content-inner"><article id="post"><div class="plain" id="post-title">TensorRT</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-05</time><span class="post-meta__separator">|</span><i class="fa fa-inbox" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/AI/"> AI</a></div><div class="article-container" id="post-content"><blockquote>
<p>TensorRT</p>
</blockquote>
<a id="more"></a>
<h1 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h1>
<h2 id="介绍"><a class="markdownIt-Anchor" href="#介绍"></a> 介绍</h2>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html">TensorRT</a>是一个高性能的深度学习推理（Inference）优化器，可以为深度学习应用提供低延迟、高吞吐率的部署推理。TensorRT可用于对超大规模数据中心、嵌入式平台或自动驾驶平台进行推理加速。TensorRT现已能支持TensorFlow、Caffe、Mxnet、Pytorch等几乎所有的深度学习框架，将TensorRT和NVIDIA的GPU结合起来，能在几乎所有的框架中进行快速和高效的部署推理。</p>
</li>
<li>
<p>TensorRT 是一个C<ins>库，从 TensorRT 3 开始提供C</ins> API和Python API，主要用来针对 NVIDIA GPU进行 高性能推理（Inference）加速。现在最新版TensorRT是4.0版本。</p>
</li>
<li>
<p>可以认为tensorRT是一个只有前向传播的深度学习框架，这个框架可以将 Caffe，TensorFlow的网络模型解析，然后与tensorRT中对应的层进行一一映射，把其他框架的模型统一全部 转换到tensorRT中，然后在tensorRT中可以针对NVIDIA自家GPU实施优化策略，并进行部署加速。</p>
</li>
<li>
<p>目前TensorRT4.0 几乎可以支持所有常用的深度学习框架，对于caffe和TensorFlow来说，tensorRT可以直接解析他们的网络模型；对于caffe2，pytorch，mxnet，chainer，CNTK等框架则是首先要将模型转为 ONNX 的通用深度学习模型，然后对ONNX模型做解析。而tensorflow和MATLAB已经将TensorRT集成到框架中去了。</p>
</li>
<li>
<p>ONNX（Open Neural Network Exchange）是微软和Facebook携手开发的开放式神经网络交换工具，也就是说不管用什么框架训练，只要转换为ONNX模型，就可以放在其他框架上面去inference。这是一种统一的神经网络模型定义和保存方式，上面提到的除了tensorflow之外的其他框架官方应该都对onnx做了支持，而ONNX自己开发了对tensorflow的支持。从深度学习框架方面来说，这是各大厂商对抗谷歌tensorflow垄断地位的一种有效方式；从研究人员和开发者方面来说，这可以使开发者轻易地在不同机器学习工具之间进行转换，并为项目选择最好的组合方式，加快从研究到生产的速度。</p>
</li>
</ul>
<p><img src="/images/pasted-271.png" alt="tensorRT" /></p>
<h2 id="支持的层"><a class="markdownIt-Anchor" href="#支持的层"></a> 支持的层</h2>
<table>
<thead>
<tr>
<th>层</th>
<th>支持</th>
</tr>
</thead>
<tbody>
<tr>
<td>Activation</td>
<td>ReLU, tanh and sigmoid</td>
</tr>
<tr>
<td>Concatenation</td>
<td>: Link together multiple tensors across the channel dimension.</td>
</tr>
<tr>
<td>Convolution</td>
<td>3D，2D</td>
</tr>
<tr>
<td>Deconvolution</td>
<td>Fully</td>
</tr>
<tr>
<td>ElementWise</td>
<td>sum, product or max of two tensors</td>
</tr>
<tr>
<td>Pooling</td>
<td>max and average</td>
</tr>
<tr>
<td>Padding</td>
<td>Flatten</td>
</tr>
<tr>
<td>SoftMax</td>
<td>cross-channel only</td>
</tr>
<tr>
<td>RNN</td>
<td>RNN, GRU, and LSTM</td>
</tr>
<tr>
<td>Scale</td>
<td>Affine transformation and/or exponentiation by constant values</td>
</tr>
<tr>
<td>Shuffle</td>
<td>Reshuffling of tensors , reshape or transpose data</td>
</tr>
<tr>
<td>Squeeze</td>
<td>Removes dimensions of size 1 from the shape of a tensor</td>
</tr>
<tr>
<td>Unary</td>
<td>Supported operations are exp, log, sqrt, recip, abs and neg</td>
</tr>
<tr>
<td>Plugin</td>
<td>integrate custom layer implementations that TensorRT does not natively support.</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>基本上比较经典的层比如，卷积，反卷积，全连接，RNN，softmax等，在tensorRT中都是有对应的实现方式的，tensorRT是可以直接解析的。</p>
</li>
<li>
<p>但是由于现在深度学习技术发展日新月异，各种不同结构的自定义层（比如：STN）层出不穷，所以tensorRT是不可能全部支持当前存在的所有层的。那对于这些自定义的层tensorRT中有一个 Plugin 层，这个层提供了 API 可以由用户自己定义tensorRT不支持的层。</p>
</li>
</ul>
<p><img src="/images/pasted-272.png" alt="TensorRT-plugin" /></p>
<h2 id="优化方式"><a class="markdownIt-Anchor" href="#优化方式"></a> 优化方式</h2>
<ul>
<li>
<p>TensorRT优化方法主要有以下几种方式，最主要的是两种：层间融合或张量融合（Layer &amp; Tensor Fusion）、数据精度校准（Weight &amp;Activation Precision Calibration）。<br />
<img src="/images/pasted-273.png" alt="TensorRT-optimize-method" /></p>
</li>
<li>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/qccz123456/p/11767858.html">详细介绍</a></p>
</li>
</ul>
<h2 id="安装"><a class="markdownIt-Anchor" href="#安装"></a> 安装</h2>
<p><strong>cuda</strong></p>
<ul>
<li>首先要在机器上安装cuda，在<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-toolkit-archive">官网</a>找到对应的系统下载安装即可。</li>
<li>完成后可以测试是否成功：<code>nvcc --version</code><br />
<strong>cudnn</strong></li>
<li>安装cudnn，<a target="_blank" rel="noopener" href="https://developer.nvidia.com/rdp/cudnn-archive">官网</a>。</li>
<li>把cudnn解压到cuda路径即可</li>
</ul>
<p><strong>配置环境变量</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/<span class="built_in">local</span>/cuda</span><br><span class="line"><span class="built_in">export</span> CUDNN_HOME=xxx/cudnn-10.0xxxx/cuda</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$CUDA_HOME</span>/bin:<span class="variable">$CUDNN_HOME</span>/bin</span><br></pre></td></tr></table></figure>
<h3 id="linux"><a class="markdownIt-Anchor" href="#linux"></a> linux</h3>
<ul>
<li><a target="_blank" rel="noopener" href="http://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html">官方指导</a></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里改为自己对应的cuda版本</span></span><br><span class="line">$ sudo dpkg -i nv-tensorrt-repo-ubuntu1604-ga-cuda8.0-trt3.0-20171128_1-1_amd64.deb</span><br><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install tensorrt</span><br><span class="line">$ sudo apt-get install python3-libnvinfer-doc</span><br><span class="line">$ sudo apt-get install uff-converter-tf</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>安装好后，使用 <code>$ dpkg -l | grep TensorRT</code> 命令检测是否成功</p>
</li>
<li>
<p>安装后会在 /usr/src 目录下生成一个 tensorrt 文件夹，里面包含 bin , data , python , samples 四个文件夹， samples 文件夹中是官方例程的源码； data , python 文件中存放官方例程用到的资源文件，比如caffemodel文件，TensorFlow模型文件，一些图片等；bin 文件夹用于存放编译后的二进制文件。</p>
</li>
<li>
<p>可以把 tensorrt 文件夹拷贝到用户目录下，方便自己修改测试例程中的代码。</p>
</li>
<li>
<p>进入 samples 文件夹直接 make，会在 bin 目录中生成可执行文件，可以一一进行测试学习。</p>
</li>
<li>
<p>另外tensorRT是不开源的， 它的头文件位于<code>/usr/include/x86_64-linux-gnu</code>目录下，共有七个</p>
</li>
<li>
<p>tensorRT的库文件位于<code>/usr/lib/x86_64-linux-gnu</code>目录下</p>
</li>
<li>
<p>环境变量</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tensorrt</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=~/TensorRT-7.0.0,11/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="comment"># cuda</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-10.2/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> CUDA_INSTALL_DIR=/usr/<span class="built_in">local</span>/cuda-10.2</span><br><span class="line"><span class="built_in">export</span> CUDNN_INSTALL_DIR=/usr/<span class="built_in">local</span>/cuda-10.2</span><br><span class="line"><span class="comment"># 复制tensorrt的lib和include到系统文件夹</span></span><br><span class="line">sudo cp -r ./lib/* /usr/lib</span><br><span class="line">sudo cp -r ./include/* /usr/include</span><br></pre></td></tr></table></figure>
<h2 id="安装pycuda"><a class="markdownIt-Anchor" href="#安装pycuda"></a> 安装pycuda</h2>
<ul>
<li>使用python接口的tensorRT，需要安装pycuda</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install pycuda</span><br></pre></td></tr></table></figure>
<ul>
<li>测试</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorrt</span><br><span class="line">tensorrt.__version__</span><br></pre></td></tr></table></figure>
<h2 id="安装opencv"><a class="markdownIt-Anchor" href="#安装opencv"></a> 安装opencv</h2>
<ul>
<li>建议使用opencv3版本，在cmake时手动指定CUDA参数</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> <span class="string">&quot;https://github.com/opencv/opencv.git&quot;</span></span><br><span class="line">git <span class="built_in">clone</span> <span class="string">&quot;https://github.com/opencv/opencv_contrib.git&quot;</span></span><br><span class="line">mkdir 20190509_cuda</span><br><span class="line">mkdir -p opencv-master/build_cuda</span><br><span class="line"><span class="built_in">cd</span> opencv-master/build_cuda</span><br><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -D INSTALL_PYTHON_EXAMPLES=ON -D INSTALL_C_EXAMPLES=OFF -DOPENCV_EXTRA_MODULES_PATH=~/opencv_contrib-3.4.4/modules -D CUDA_GENERATION=Auto -D PYTHON-EXCUTABLE=/usr/bin/python -D WITH_TBB=ON =D WITH_V4L=ON -D WITH_GTK=ON -D WITH_OPENGL=ON -D BUILD_EXAMPLES=ON -DCUDA_CUDA_LIBRARY=/usr/<span class="built_in">local</span>/cuda-9.0/lib64/stubs/libcuda.so -DCMAKE_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-9.0/lib64/stubs/ ..</span><br><span class="line">ccmake ..</span><br><span class="line"><span class="comment">#按t，切换进入高级模式</span></span><br><span class="line">make -j 20</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<ul>
<li>配置</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">//Build CUDA modules stubs when no CUDA SDK</span><br><span class="line">BUILD_CUDA_STUBS:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Create build rules <span class="keyword">for</span> OpenCV Documentation</span><br><span class="line">BUILD_DOCS:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build all examples</span><br><span class="line">BUILD_EXAMPLES:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Create Java wrapper exporting all <span class="built_in">functions</span> of OpenCV library</span><br><span class="line">// (requires static build of OpenCV modules)</span><br><span class="line">BUILD_FAT_JAVA_LIB:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build IPP IW from <span class="built_in">source</span></span><br><span class="line">BUILD_IPP_IW:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build Intel ITT from <span class="built_in">source</span></span><br><span class="line">BUILD_ITT:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build libjasper from <span class="built_in">source</span></span><br><span class="line">BUILD_JASPER:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Enable Java support</span><br><span class="line">BUILD_JAVA:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build libjpeg from <span class="built_in">source</span></span><br><span class="line">BUILD_JPEG:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build openexr from <span class="built_in">source</span></span><br><span class="line">BUILD_OPENEXR:BOOL=ON</span><br><span class="line"></span><br><span class="line">//Include ILM support via OpenEXR</span><br><span class="line">WITH_OPENEXR:BOOL=ON</span><br><span class="line"></span><br><span class="line">//Build performance tests</span><br><span class="line">BUILD_PERF_TESTS:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build libpng from <span class="built_in">source</span></span><br><span class="line">BUILD_PNG:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Force to build libprotobuf from sources</span><br><span class="line">BUILD_PROTOBUF:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build shared libraries (.dll/.so) instead of static ones (.lib/.a)</span><br><span class="line">BUILD_SHARED_LIBS:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Download and build TBB from <span class="built_in">source</span></span><br><span class="line">BUILD_TBB:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build accuracy &amp; regression tests</span><br><span class="line">BUILD_TESTS:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build libtiff from <span class="built_in">source</span></span><br><span class="line">BUILD_TIFF:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build WebP from <span class="built_in">source</span></span><br><span class="line">BUILD_WEBP:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include debug info into release binaries (<span class="string">&#x27;OFF&#x27;</span> means default</span><br><span class="line">// settings)</span><br><span class="line">BUILD_WITH_DEBUG_INFO:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Enables dynamic linking of IPP (only <span class="keyword">for</span> standalone IPP)</span><br><span class="line">BUILD_WITH_DYNAMIC_IPP:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Build zlib from <span class="built_in">source</span></span><br><span class="line">BUILD_ZLIB:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include opencv_face module into the OpenCV build</span><br><span class="line">BUILD_opencv_face:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include opencv_highgui module into the OpenCV build</span><br><span class="line">BUILD_opencv_highgui:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include opencv_java_bindings_generator module into the OpenCV</span><br><span class="line">// build</span><br><span class="line">BUILD_opencv_java_bindings_generator:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include opencv_xfeatures2d module into the OpenCV build</span><br><span class="line">BUILD_opencv_xfeatures2d:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Choose the <span class="built_in">type</span> of build, options are: None Debug Release RelWithDebInfo</span><br><span class="line">// MinSizeRel ...</span><br><span class="line">CMAKE_BUILD_TYPE:STRING=Debug</span><br><span class="line"></span><br><span class="line">//Installation Directory</span><br><span class="line">CMAKE_INSTALL_PREFIX:PATH=/home/admin/opencv/2019-05-09_cuda</span><br><span class="line"></span><br><span class="line">//<span class="string">&quot;cudart&quot;</span> library</span><br><span class="line">CUDA_CUDART_LIBRARY:FILEPATH=/usr/<span class="built_in">local</span>/cuda/lib64/libcudart.so</span><br><span class="line"></span><br><span class="line">//Path to a program.</span><br><span class="line">CUDA_NVCC_EXECUTABLE:FILEPATH=/usr/<span class="built_in">local</span>/cuda/bin/nvcc</span><br><span class="line"></span><br><span class="line">//Path to a file.</span><br><span class="line">CUDA_TOOLKIT_INCLUDE:PATH=/usr/<span class="built_in">local</span>/cuda/include</span><br><span class="line"></span><br><span class="line">//Toolkit location.</span><br><span class="line">CUDA_TOOLKIT_ROOT_DIR:PATH=/usr/<span class="built_in">local</span>/cuda</span><br><span class="line"></span><br><span class="line">//Use the static version of the CUDA runtime library <span class="keyword">if</span> available</span><br><span class="line">CUDA_USE_STATIC_CUDA_RUNTIME:BOOL=ON</span><br><span class="line"></span><br><span class="line">//Enable ADE framework (required <span class="keyword">for</span> Graph API module)</span><br><span class="line">WITH_ADE:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include Aravis GigE support</span><br><span class="line">WITH_ARAVIS:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include NVidia Cuda Runtime support</span><br><span class="line">WITH_CUDA:BOOL=ON</span><br><span class="line"></span><br><span class="line">//Include Intel Inference Engine support</span><br><span class="line">WITH_INF_ENGINE:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include Intel IPP support</span><br><span class="line">WITH_IPP:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include Intel ITT support</span><br><span class="line">WITH_ITT:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include JPEG2K support</span><br><span class="line">WITH_JASPER:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include JPEG support</span><br><span class="line">WITH_JPEG:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include Lapack library support</span><br><span class="line">WITH_LAPACK:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include NVidia Video Decoding library support</span><br><span class="line">WITH_NVCUVID:BOOL=ON</span><br><span class="line"></span><br><span class="line">//Include PNG support</span><br><span class="line">WITH_PNG:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Enable libprotobuf</span><br><span class="line">WITH_PROTOBUF:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include Intel TBB support</span><br><span class="line">WITH_TBB:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include TIFF support</span><br><span class="line">WITH_TIFF:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include Vulkan support</span><br><span class="line">WITH_VULKAN:BOOL=OFF</span><br><span class="line"></span><br><span class="line">//Include WebP support</span><br><span class="line">WITH_WEBP:BOOL=OFF</span><br></pre></td></tr></table></figure>
<h3 id="windows"><a class="markdownIt-Anchor" href="#windows"></a> windows</h3>
<ul>
<li>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/tensorrt">官方网址</a>，<a target="_blank" rel="noopener" href="https://developer.nvidia.com/nvidia-tensorrt-7x-download">下载安装包</a></p>
</li>
<li>
<p>解压，设置系统环境变量</p>
</li>
<li>
<p>复制dll文件到cuda安装目录</p>
</li>
<li>
<p>完成tensorRT安装后，测试看安装是否成功，可以直接编译刚才解压的TensorRT里的案例来测试。这里我们选用sampleMNIST来测试。<a target="_blank" rel="noopener" href="https://blog.csdn.net/yangzzguang/article/details/85570663">流程</a></p>
</li>
</ul>
<h1 id="功能介绍"><a class="markdownIt-Anchor" href="#功能介绍"></a> 功能介绍</h1>
<h2 id="流程"><a class="markdownIt-Anchor" href="#流程"></a> 流程</h2>
<table>
<thead>
<tr>
<th>流程</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型训练</td>
<td>导出得到模型权重文件，例如onnx、tensorflow、caffe等</td>
</tr>
<tr>
<td>builder</td>
<td>模型优化器，构建器将网络模型定义作为输入，执行与设备无关和针对特定设备的优化，并创建引擎</td>
</tr>
<tr>
<td>network definition</td>
<td>模型表示，定义网络张量和运算符的图</td>
</tr>
<tr>
<td>engine</td>
<td>由构建器优化的模型表示，推理引擎用于序列化和反序列化，保存和解析构建的优化模型</td>
</tr>
<tr>
<td>plan</td>
<td>计划文件，序列化格式的推理引擎。初始化推理引擎，应用程序首先从plan文件中反序列化模型</td>
</tr>
<tr>
<td>runtime</td>
<td>执行推理，输入数据即可得到输出结果</td>
</tr>
</tbody>
</table>
<ul>
<li>build阶段会根据网络定义，执行包括平台特定优化，生成特定的推理引擎。这个阶段可能会花费大量时间，所以将其序列化为plan文件以供以后使用</li>
<li>其中构建时的优化包括：
<ul>
<li>消除不使用的输出层</li>
<li>消除等同于无操作的内容</li>
<li>卷积、偏置和relu操作融合</li>
<li>使用完全相似的参数和相同的源的张量进行操作聚合，例如gooleNetv5的初始模块汇总1x1卷积</li>
<li>将层输出定向到正确的最终目的地来合并拼接层</li>
</ul>
</li>
</ul>
<h2 id="工作流"><a class="markdownIt-Anchor" href="#工作流"></a> 工作流</h2>
<table>
<thead>
<tr>
<th>工作流</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>导出模型</td>
<td></td>
</tr>
<tr>
<td>设定batch size</td>
<td>推断时的batch size</td>
</tr>
<tr>
<td>设定精度</td>
<td>推理时使用较低精度提供更快的计算速度和更低的内存消耗。支持TF32、FP32、FP16、INT8精度</td>
</tr>
<tr>
<td>模型转换</td>
<td>模型从onnx、wts等转换为tensorrt引擎。三种方式：1、使用TF-TRT；2、onnx文件自动转换；3、使用tensorrt API手动创建网络(最高的性能)</td>
</tr>
<tr>
<td>部署模型</td>
<td>两种类型的runtime，C++和python绑定的独立runtime，以及tensorflow的原生集成的runtime。三种方式：1、tensorflow部署；2、使用tensorrt runtime API部署；3、使用NVIDIA推理服务器部署；</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>精度范围</th>
<th>动态范围</th>
<th>最小精度</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>3.4</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>38</mn></msup><mtext> </mtext><mn>3.4</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>38</mn></msup></mrow><annotation encoding="application/x-tex">-3.4\times 10^{38} ~ 3.4\times 10^{38}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">3</span><span class="mord">.</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">8</span></span></span></span></span></span></span></span></span><span class="mspace nobreak"> </span><span class="mord">3</span><span class="mord">.</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></td>
<td><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1.4</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>45</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1.4\times 10^{-45}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">4</span><span class="mord mtight">5</span></span></span></span></span></span></span></span></span></span></span></span></td>
</tr>
<tr>
<td>FP16</td>
<td><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>65504</mn><mtext> </mtext><mn>65504</mn></mrow><annotation encoding="application/x-tex">-65504~65504</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">6</span><span class="mord">5</span><span class="mord">5</span><span class="mord">0</span><span class="mord">4</span><span class="mspace nobreak"> </span><span class="mord">6</span><span class="mord">5</span><span class="mord">5</span><span class="mord">0</span><span class="mord">4</span></span></span></span></td>
<td><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>5.96</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5.96\times 10^{-8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mord">.</span><span class="mord">9</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></td>
</tr>
<tr>
<td>INT8</td>
<td><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>−</mo><mn>128</mn><mtext> </mtext><mn>127</mn></mrow><annotation encoding="application/x-tex">-128~127</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mord">2</span><span class="mord">8</span><span class="mspace nobreak"> </span><span class="mord">1</span><span class="mord">2</span><span class="mord">7</span></span></span></span></td>
<td><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></td>
</tr>
</tbody>
</table>
<ul>
<li>检查GPU是否支持FP16/INT8，<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-gpus#compute">GPU精度能力</a>，<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware=precision-matrix">GPU精度矩阵</a></li>
</ul>
<h1 id="使用"><a class="markdownIt-Anchor" href="#使用"></a> 使用</h1>
<h2 id="创建网络"><a class="markdownIt-Anchor" href="#创建网络"></a> 创建网络</h2>
<h3 id="原始api搭建"><a class="markdownIt-Anchor" href="#原始api搭建"></a> 原始api搭建</h3>
<ul>
<li>创建builder和网络</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class="line">INetworkDefinition* network = builder-&gt;createNetworkV2(<span class="number">1U</span> &lt;&lt; <span class="keyword">static_cast</span>&lt;<span class="keyword">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br></pre></td></tr></table></figure>
<ul>
<li>定义网络输入输出</li>
</ul>
<blockquote>
<p>Add the Input layer to the network, with the input dimensions, including dynamic batch. A network can have multiple inputs, although in this sample there is only one:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> data = network-&gt;addInput(INPUT_BLOB_NAME, dt, Dims3&#123;<span class="number">-1</span>, <span class="number">1</span>, INPUT_H, INPUT_W&#125;);</span><br></pre></td></tr></table></figure>
<ul>
<li>添加卷积层</li>
</ul>
<blockquote>
<p>Add the Convolution layer with hidden layer input nodes, strides and weights for filter and bias. In order to retrieve the tensor reference from the layer, we can use:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> conv1 = network-&gt;addConvolution(*data-&gt;getOutput(<span class="number">0</span>), <span class="number">20</span>, DimsHW&#123;<span class="number">5</span>, <span class="number">5</span>&#125;, weightMap[<span class="string">&quot;conv1filter&quot;</span>], weightMap[<span class="string">&quot;conv1bias&quot;</span>]);</span><br><span class="line">conv1-&gt;setStride(DimsHW&#123;<span class="number">1</span>, <span class="number">1</span>&#125;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: Weights passed to TensorRT layers are in host memory.</p>
</blockquote>
<ul>
<li>添加池化层</li>
</ul>
<blockquote>
<p>Add the Pooling layer:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> pool1 = network-&gt;addPooling(*conv1-&gt;getOutput(<span class="number">0</span>), PoolingType::kMAX, DimsHW&#123;<span class="number">2</span>, <span class="number">2</span>&#125;);</span><br><span class="line">pool1-&gt;setStride(DimsHW&#123;<span class="number">2</span>, <span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure>
<ul>
<li>全连接和激活层</li>
</ul>
<blockquote>
<p>Add the FullyConnected and Activation layers:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> ip1 = network-&gt;addFullyConnected(*pool1-&gt;getOutput(<span class="number">0</span>), <span class="number">500</span>, weightMap[<span class="string">&quot;ip1filter&quot;</span>], weightMap[<span class="string">&quot;ip1bias&quot;</span>]);</span><br><span class="line"><span class="keyword">auto</span> relu1 = network-&gt;addActivation(*ip1-&gt;getOutput(<span class="number">0</span>), ActivationType::kRELU);</span><br></pre></td></tr></table></figure>
<ul>
<li>softmax层</li>
</ul>
<blockquote>
<p>Add the SoftMax layer to calculate the final probabilities and set it as the output:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> prob = network-&gt;addSoftMax(*relu1-&gt;getOutput(<span class="number">0</span>));</span><br><span class="line">prob-&gt;getOutput(<span class="number">0</span>)-&gt;setName(OUTPUT_BLOB_NAME);</span><br></pre></td></tr></table></figure>
<ul>
<li>输出</li>
</ul>
<blockquote>
<p>Mark the output:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">network-&gt;markOutput(*prob-&gt;getOutput(<span class="number">0</span>));</span><br></pre></td></tr></table></figure>
<h3 id="通过已生成网络搭建"><a class="markdownIt-Anchor" href="#通过已生成网络搭建"></a> 通过已生成网络搭建</h3>
<ul>
<li>parse解析器支持：ONNX、UFF、Caffe的方式，导入网络模型</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//ONNX: </span></span><br><span class="line"><span class="keyword">auto</span> parser = nvonnxparser::createParser(*network, gLogger);</span><br><span class="line"><span class="comment">//Caffe: </span></span><br><span class="line"><span class="keyword">auto</span> parser = nvcaffeparser1::createCaffeParser();</span><br><span class="line"><span class="comment">//UFF: </span></span><br><span class="line"><span class="keyword">auto</span> parser = nvuffparser::createUffParser();</span><br></pre></td></tr></table></figure>
<ul>
<li>解析Caffe网络</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//Create the builder and network:</span></span><br><span class="line">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class="line">INetworkDefinition* network = builder-&gt;createNetworkV2(<span class="number">0U</span>);</span><br><span class="line"><span class="comment">//Create the Caffe parser:</span></span><br><span class="line">ICaffeParser* parser = createCaffeParser();</span><br><span class="line">Parse the imported model:</span><br><span class="line"><span class="keyword">const</span> IBlobNameToTensor* blobNameToTensor = parser-&gt;parse(<span class="string">&quot;deploy_file&quot;</span> , <span class="string">&quot;modelFile&quot;</span>, *network, DataType::kFLOAT);</span><br><span class="line"><span class="comment">//Specify the outputs of the network:</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span>&amp; s : outputs)</span><br><span class="line">    network-&gt;markOutput(*blobNameToTensor-&gt;find(s.c_str()));</span><br></pre></td></tr></table></figure>
<h2 id="创建engine"><a class="markdownIt-Anchor" href="#创建engine"></a> 创建engine</h2>
<blockquote>
<p>Build the engine using the builder object:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">IBuilderConfig* config = builder-&gt;createBuilderConfig();</span><br><span class="line">config-&gt;setMaxWorkspaceSize(<span class="number">1</span> &lt;&lt; <span class="number">20</span>);</span><br><span class="line">ICudaEngine* engine = builder-&gt;buildEngineWithConfig(*network, *config);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>When the engine is built, TensorRT makes copies of the weights.</p>
</blockquote>
<ul>
<li>释放资源</li>
</ul>
<blockquote>
<p>Dispense with the network, builder, and parser if using one.</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">parser-&gt;destroy();</span><br><span class="line">network-&gt;destroy();</span><br><span class="line">config-&gt;destroy();</span><br><span class="line">builder-&gt;destroy();</span><br></pre></td></tr></table></figure>
<h2 id="序列化模型"><a class="markdownIt-Anchor" href="#序列化模型"></a> 序列化模型</h2>
<ul>
<li>序列化模型</li>
</ul>
<blockquote>
<p>Run the builder as a prior offline step and then serialize:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">IHostMemory *serializedModel = engine-&gt;serialize();</span><br><span class="line"><span class="comment">// store model to disk</span></span><br><span class="line"><span class="comment">// &lt;…&gt;</span></span><br><span class="line">serializedModel-&gt;destroy();</span><br></pre></td></tr></table></figure>
<ul>
<li>反序列化</li>
</ul>
<blockquote>
<p>Create a runtime object to deserialize:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">IRuntime* runtime = createInferRuntime(gLogger);</span><br><span class="line">ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(modelData, modelSize, <span class="literal">nullptr</span>);</span><br></pre></td></tr></table></figure>
<h2 id="执行模型"><a class="markdownIt-Anchor" href="#执行模型"></a> 执行模型</h2>
<blockquote>
<p>Create some space to store intermediate activation values. Since the engine holds the network definition and trained parameters, additional space is necessary. These are held in an execution context:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">IExecutionContext *context = engine-&gt;createExecutionContext();</span><br></pre></td></tr></table></figure>
<blockquote>
<p>An engine can have multiple execution contexts, allowing one set of weights to be used for multiple overlapping inference tasks. For example, you can process images in parallel CUDA streams using one engine and one context per stream. Each context will be created on the same GPU as the engine.</p>
</blockquote>
<blockquote>
<p>Use the input and output blob names to get the corresponding input and output index:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> inputIndex = engine-&gt;getBindingIndex(INPUT_BLOB_NAME);</span><br><span class="line"><span class="keyword">int</span> outputIndex = engine-&gt;getBindingIndex(OUTPUT_BLOB_NAME);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Using these indices, set up a buffer array pointing to the input and output buffers on the GPU:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span>* buffers[<span class="number">2</span>];</span><br><span class="line">buffers[inputIndex] = inputBuffer;</span><br><span class="line">buffers[outputIndex] = outputBuffer;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>TensorRT execution is typically asynchronous, so enqueue the kernels on a CUDA stream:</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">context-&gt;enqueueV2(buffers, stream, <span class="literal">nullptr</span>);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>It is common to enqueue asynchronous memcpy() before and after the kernels to move data from the GPU if it is not already there. The final argument to enqueueV2() is an optional CUDA event which will be signaled when the input buffers have been consumed and their memory may be safely reused.</p>
</blockquote>
<blockquote>
<p>To determine when the kernel (and possibly memcpy()) are complete, use standard CUDA synchronization mechanisms such as events, or waiting on the stream.</p>
</blockquote>
<h1 id="int8加速"><a class="markdownIt-Anchor" href="#int8加速"></a> INT8加速</h1>
<h2 id="流程-2"><a class="markdownIt-Anchor" href="#流程-2"></a> 流程</h2>
<ol>
<li>准备校准图片（calibration images），保存在<code>tensorrtx/yolov5/build</code></li>
<li>yolov5.cpp设置宏：<code>#define USE_INT8</code></li>
<li><code>make &amp;&amp; sudo ./yolov5 -s &amp;&amp; sudo ./yolov5 -d ../samples</code></li>
</ol>
<h1 id="示例"><a class="markdownIt-Anchor" href="#示例"></a> 示例</h1>
<ul>
<li>下面代码是一个简单的build过程</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">IBuilder* builder = createInferBuilder(gLogger);</span><br><span class="line"><span class="comment">// parse the caffe model to populate the network, then set the outputs</span></span><br><span class="line"><span class="comment">// 创建一个network对象，不过这时network对象只是一个空架子</span></span><br><span class="line">INetworkDefinition* network = builder-&gt;createNetwork();</span><br><span class="line"><span class="comment">//tensorRT提供一个高级别的API：CaffeParser，用于解析Caffe模型</span></span><br><span class="line"><span class="comment">//parser.parse函数接受的参数就是上面提到的文件，和network对象</span></span><br><span class="line"><span class="comment">//这一步之后network对象里面的参数才被填充，才具有实际的意义</span></span><br><span class="line">CaffeParser parser;</span><br><span class="line"><span class="keyword">auto</span> blob_name_to_tensor = parser.parse(“deploy.prototxt”,trained_file.c_str(),*network,DataType::kFLOAT);</span><br><span class="line"><span class="comment">// 标记输出 tensors</span></span><br><span class="line"><span class="comment">// specify which tensors are outputs</span></span><br><span class="line">network-&gt;markOutput(*blob_name_to_tensor-&gt;find(<span class="string">&quot;prob&quot;</span>));</span><br><span class="line"><span class="comment">// Build the engine</span></span><br><span class="line"><span class="comment">// 设置batchsize和工作空间，然后创建inference engine</span></span><br><span class="line">builder-&gt;setMaxBatchSize(<span class="number">1</span>);</span><br><span class="line">builder-&gt;setMaxWorkspaceSize(<span class="number">1</span> &lt;&lt; <span class="number">30</span>);</span><br><span class="line"><span class="comment">//调用buildCudaEngine时才会进行前述的层间融合或精度校准优化方式</span></span><br><span class="line">ICudaEngine* engine = builder-&gt;buildCudaEngine(*network);</span><br></pre></td></tr></table></figure>
<h1 id="gpu编程"><a class="markdownIt-Anchor" href="#gpu编程"></a> GPU编程</h1>
<ul>
<li>由于GPU处理核心更多，在处理并行运算时比CPU更高效。</li>
</ul>
<table>
<thead>
<tr>
<th>CPU</th>
<th>GPU</th>
<th>层次</th>
</tr>
</thead>
<tbody>
<tr>
<td>算术逻辑和控制单元</td>
<td>流处理器SM</td>
<td>硬件</td>
</tr>
<tr>
<td>算术单元</td>
<td>批量处理器SP</td>
<td>硬件</td>
</tr>
<tr>
<td>进程</td>
<td>block</td>
<td>软件</td>
</tr>
<tr>
<td>线程</td>
<td>thread</td>
<td>软件</td>
</tr>
<tr>
<td>调度单位</td>
<td>warp</td>
<td>软件</td>
</tr>
</tbody>
</table>
<h2 id="c"><a class="markdownIt-Anchor" href="#c"></a> C</h2>
<ul>
<li><code>cuda C</code>是对常规c的扩展，加入了一些函数前缀</li>
</ul>
<table>
<thead>
<tr>
<th>前缀</th>
<th>执行位置</th>
<th>调用位置</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>device</strong> float DeviceFunc()</td>
<td>device</td>
<td>device</td>
</tr>
<tr>
<td><strong>global</strong> float KernelFunc()</td>
<td>device</td>
<td>host</td>
</tr>
<tr>
<td><strong>host</strong> float HostFunc()</td>
<td>host</td>
<td>host</td>
</tr>
</tbody>
</table>
<p><img src="/images/pasted_398.png" alt="" /></p>
<ul>
<li>
<p>一个grid线程两个内置的坐标变量：blockIdx和threadIdx</p>
</li>
<li>
<p>barrier同步等待，由于并发特性，程序里用到计算结果时，等待所有kernel计算完成再继续</p>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>七个步骤</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cudaSetDevice(0);</code></td>
<td>获取设备，默认0，只有一个GPU可以省略</td>
</tr>
<tr>
<td><code>cudaMalloc((void**))&amp; d_a,sizeof(float)*n);</code></td>
<td>分配显存</td>
</tr>
<tr>
<td><code>cudaMemcpy(d_a,a,sizeof(float)*n,cudaMemcpyHostToDevice);</code></td>
<td>数据传输至GPU</td>
</tr>
<tr>
<td><code>gpu_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(***);</code></td>
<td>kernel函数</td>
</tr>
<tr>
<td><code>cudaMemcpy(a,d_a,sizeof(float)*n,cudaMemcpyDeivceToHost);</code></td>
<td>数据传输至CPU</td>
</tr>
<tr>
<td><code>cudaFree(d_a);</code></td>
<td>释放显存</td>
</tr>
<tr>
<td><code>cudaDeviceReset();</code></td>
<td>重置设备，可以省略</td>
</tr>
</tbody>
</table>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//kernel definition</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">VecAdd</span><span class="params">(<span class="keyword">float</span>* A, <span class="keyword">float</span>* B, <span class="keyword">float</span>* C)</span></span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i=threadIdx.x;</span><br><span class="line">  C[i]=A[i]+B[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">//1,N表示分配了一个线程快block，每个线程块分配了N个线程</span></span><br><span class="line">  VecAdd&lt;&lt;&lt;<span class="number">1</span>,N&gt;&gt;&gt;(A,B,C);</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="cpp"><a class="markdownIt-Anchor" href="#cpp"></a> CPP</h2>
<h1 id="实践"><a class="markdownIt-Anchor" href="#实践"></a> 实践</h1>
<h2 id="yolov5"><a class="markdownIt-Anchor" href="#yolov5"></a> yolov5</h2>
<ul>
<li>使用tensorrt API搭建的yolov5网络生成的wts文件，<a target="_blank" rel="noopener" href="https://github.com/wang-xinyu/tensorrtX">yolov-tensorrtX</a></li>
<li>生成wts文件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python gen_wts.py <span class="comment"># 将gen_wts.py放在yolov5文件夹下，再将生成的yolov5.wts，放在tensorrtx/yolov5文件夹下</span></span><br></pre></td></tr></table></figure>
<ul>
<li>编译前修改yolov5.cpp的类型<code>#define Net s</code>：可以为s,m,l,x等，yololayer.h文件下类别个数CLASS_NUM</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<ul>
<li>执行tensorRT加速后的yolov5</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ./yolov5 -s <span class="comment"># 序列化</span></span><br><span class="line">sudo ./yolov5 -d ./samples <span class="comment"># 反序列化，并推理</span></span><br></pre></td></tr></table></figure>
<ul>
<li>使用python执行加速</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python yolov5_trt.py</span><br></pre></td></tr></table></figure>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text/javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.css"></div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/algorithm/">algorithm</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/01/02/decorator/"><i class="fa fa-chevron-left">  </i><span>装饰器</span></a></div><div class="next-post pull-right"><a href="/2020/11/29/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span>三维重建</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == 'true';
var verify = 'false' == 'true';
var record_ip = '' == 'true';
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  recordIP:record_ip,
  appId:'AdfkiqY89QUSUWbDY9xJuCh0-gzGzoHsz',
  appKey:'2cEvHcqEWsyoKwy4AUL3kPGh',
  placeholder:'劈个叉吧',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'100',
  lang: 'zh-cn'
})</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2021 By hero576</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.2"></script><script src="/js/fancybox.js?version=1.8.2"></script><script src="/js/sidebar.js?version=1.8.2"></script><script src="/js/copy.js?version=1.8.2"></script><script src="/js/fireworks.js?version=1.8.2"></script><script src="/js/transition.js?version=1.8.2"></script><script src="/js/scroll.js?version=1.8.2"></script><script src="/js/head.js?version=1.8.2"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>