title: 深度学习
author: hero576
tags:
  - algorithm
categories:
  - AI
date: 2020-08-25 20:03:00
---
> 深度学习

<!--more-->
# 简介
## 介绍

|名称|说明|应用|
|-|-|-|
|ANN|人工神经网络|计算机神经科学|
|CNN|卷积神经网络|图像处理|
|RNN|递归神经网络|语音识别|
|DNN|深度神经网络|声学模型|
|DBN|深度信念网络|新药研发|


# 卷积神经网络CNN
- 卷积神经网络CNN (Convolutional neural network)，用于图像识别，分割
- 思想：局部连接、全局贡献
`input -> (convolution+relu -> pooling)*n -> flatten -> full connect -> softmax -> output`

## 卷积运算
- 连续形式：$f(t)*g(t)=\int_{-\infty }^{\infty } f(\tau)g(t-\tau)d\tau$

![卷积运算](/images/pasted-108.png)

- 离散形式：$f(t)*g(t)=\sum^{\infty}_{m=-\infty}f(m)g(n-m)$

![卷积运算](/images/pasted-109.png)

- 卷积和互相关运算非常相似，区别就是卷积进行了反转，实际中就用互相关就可以。

## 概念
- 感受野：卷积窗口的大小是CNN感受野（receptive field）的一个度量
- 输出特征矩阵size：$o=\frac{W-K+2P}{S}+1$
  - O: the output height/length,
  - 𝑊: the input height/length
  - 𝐾: the filter size 
  - 𝑃: the padding 填充
  - 𝑆: the stride 步幅

### 卷积核参数共享
- 卷积核在扫原图时，参数是共享的，所以对于两个平移图像是都能感知的。所以具有平移不变性。

### im2col
- 以上我们已经知道了卷积是如何操作的，im2col的作用就是优化卷积运算

## col2im

## 池化层
- 分为：平均池化，最大池化
- 没有要学习的参数，对微小偏差具有鲁棒性


## CNN网络模型
### 图像分类
#### 数据集

数据集|说明
-|-
[MNIST](http://yann.lecun.com/exdb/mnist/)|手写体识别0-9，`28*28`灰度图，训练样本数60000，测试样本数10000
[CIFAR](http://www.cs.toronto.edu/~kriz/cifar.html)|10个类别，`32*32`彩色图像，训练样本数50000，测试样本数10000
[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)|德国Zalando公司提供的
衣物图像数据集，`28*28`灰度图，训练样本数60000，测试样本数10000
[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)|PASCAL VOC竞赛提供的数据集，包含了20类的物体，主要任务是分类，检测，分割，人体布局，人体动作识别
[ImageNet](http://www.image-net.org/)|计算机视觉数据集，包含14,197,122个张图片， 21,841个Synset索引。它一直是评估图像分类算法性能的基准。
[WebVision]()|主要有Google和Flickr两个数据源，主要是利用ImageNet 1000个类的文本信息从网站上爬数据，所以它的数据类别与ImageNet完全一样，为1000类别，由240万幅图片构成训练数据。比ImageNet的两倍还多，分别由5万张图片构成验证集和测试集（均带有人工标注）

### LeNet-5
- [LeNet-5模型](http://yann.lecun.com/exdb/lenet/)是Yann LeCun教授于1998年在论文Gradient-based learning applied to document recognition中提出。它是第一个成功应用于手写数字识别问题并产生实际商业价值的卷积神经网络

![LeNet-5网络结构](/images/pasted-118.png)

- 2个卷积层 (C1, C3), 2个下采样(池化) 层 (S2 and S4), 2个全连接层 (C5,F6), 随后是输出层


#### AlexNet 

- [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)，2012Alex和Hinton在LeNet基础上进行了更宽更深的网络设计。
- 在ILSVRC2012图像分类竞赛第一名，标志着深度学习革命的开始。

![AlexNet](/images/pasted-119.png)

##### 网络架构

![AlexNet](/images/pasted-120.png)

- AlextNet网络采用卷积神经网络架构： 5个卷积层和3个全连接层，输出层为Softmax层（识别1000类别）

id|层|输入|结构|输出|参数个数|算力
-|-|-|-|-|-|-
1|conv1|输入图像`224*224*3`|filter:96个`11*11` ，stride=4，(224-11)/4+1=54.2，所以pad=3|`55*55*96`|`(11*11*3+1)*96=35K`|`55*55*96*(11*11*3+1)=105M`
2|pool1|`55*55*96`|maxpool:`3*3`，stride=2，`(55-3)/2+1=27`|`27*27*96`|0|0
3|conv2|`27*27*96`|256个5*5kernals，pad=2,`(27-5+2*2)/1+1=27`|`27*27*256`|`(5*5*96+1)*256=307k`|`233M`
4|pool2|`27*27*256`|maxpool:`3*3`，stride=2，`(27-3)/2+1=13`|`13*13*256`|0|0
5|conv3|`13*13*256`|filter:384个`3*3`,pad=1，`(13-3+1*2)/1+1=13`|`13*13*384`|`(3*3*256+1)*384=884k`|`149M`
6|conv4|`13*13*384`|filter:384个`3*3`，pad=1，`(13-3+1*2)/1+1=13`|`13*13*384`|`(3*3*384+1)*384=1.3M`|`112M`
7|conv5|`13*13*384`|filter:256个`3*3`，pad=1，`(13-3+1*2)/1+1=13`|`13*13*256`|`(3*3*384+1)*256=442K`|`74M`
8|pool5|`13*13*256`|maxpool:`3*3`，stride=2，`(13-3)/2+1=6`|`6*6*256`|0|0
9|fullconnect1|`6*6*256=9216`|layer:4096|`4096`|`(9216+1)*4096=37M`|`37M`
10|fullconnect2|`4096`|layer:4096|`4096`|`(4096+1)*4096=16M`|`16M`
11|fullconnect3|`4096`|layer:1000|`1000`|`(4096+1)*1000=4M`|`4M`
12|softmax|`1000`||

##### 网络改进
- 首次引入了ReLU作为CNN 的激活函数，并验证其效果在较深的网络超过了Sigmoid，解决了Sigmoid在网络较深时的梯度弥散问题，提高了网络的训练速率。
- 首次引入了Dropout随机忽略部分神经元，避免过拟合。随机系数0.5，也就是随机忽略一半的神经元。只在前两个全连接层使用。
- 利用数据增强减低过拟合。利用随机裁剪和翻转镜像操作增加训练数据量。
  1. 图像平移与水平反射（镜像）:测试时，网络剪裁5个224×224图块（4个角图块和1个中心图块）以及它们的水平反射（总共10个）进行预测，并对网络的softmax层的预测求10个图块平均值。如果没有该方案， AlexNet网络会遭受严重的过拟合，这将迫使使用更小的网络
  2. 改变训练集RGB通道的图像像素强度(intensity)：对每一RGB像素$I_{x,y}=[I_{xy}^R,I_{xy}^G,I_{xy}^B]$，加上$[p_1,p_2,p_3][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]^T$，其中pi,λ是像素RGB的`3*3`协方差矩阵的特征向量和特征值。α均值为0，标准差0.1的随机变量。该方法即增加了目标对光照强度和颜色变化的鲁棒性，把top-1错误率减少了1%.
- 使用重叠最大池化(Overlapping max pooling)。最大池化可以避免平均池化的模糊化效果，而采用重叠技巧可提升特征的丰富性。每次移动的步幅小于池化的边长。
- 提出了局部响应归一化(Local Response Normalization,LRN) 层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。
- 双GPU并行计算，在第三个卷积层Conv3和全连接层做信息交互。利用GPU的并行计算能力加速网络训练过程，并采用GPU分块训练的方式解决显存对网络规模的限制

![双GPU并行计算](/images/pasted-121.png)

###### LRN
- 模拟在神经生物学中有一个概念叫做“ 侧抑制(Lateral inhibition)”。其含义是被激活的神经元会对相邻的神经元产生抑制作用。对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。

![LRN](/images/pasted-122.png)

- 算法功能是在池化之后，对通道间的数据进行了距离相关函数的归一化。

- $$b_{x,y}^{i}=\frac{a_{i}^{x,y}}{(k+\alpha\sum_{min(N-1,i+n/2)}^{j=max(0,i-n/2)}{(a_{x,y}^{j})^2})^\beta}$$

  - $a_{x,y}^{j}$:第i个卷积核在位置(𝑥, 𝑦)运用ReLU后的特征图上的输出
  - $b_{x,y}^{i}$: LRN的输出，也是下一层的输入
  - $n$:同一位置上邻近深度卷积核的数目（沿通道方向); 自己决定; AlexNet选择5
  - $N$:卷积核的总数目
  - $k,\alpha,\beta$:都是超参。 AlexNet选择 k = 2, α = 10e-4, β = 0.75

- 改变超参数可以实现其它归一化操作，如当k = 0, n=N, α = 1, β = 0.5便是经典的𝑙2归一化
- 2015年VGGNet论文Very Deep Convolutional Networks for Large-Scale Image Recognition提到LRN对VGGNet作用不大

##### 损失函数
- Softmax回归：是逻辑回归对处理多个类别的情况的推广。优化目标，相当于在预测分布下最大化训练样本中正确标签的对数概率的平均值

$$J(\theta)=-\frac{1}{m}\left [\sum_{i=1}^{m}\sum_{j=1}^{k}l(y^{(i)}=j)\log{\frac{e^{\theta^{T}_{j}x^{(i)}}}{\sum_{1\le l\le k}{\theta^{T}_{j}x^{(i)}}}} \right ]$$

- 采用独热One-hot标签编码时，目标误差函数等同于交叉熵损失

##### 网络训练
- 改进随机梯度下降
$$v_{i+1}=0.9v_i-0.0005\epsilon \omega_i-\epsilon\left \langle \frac{\partial L}{\partial \omega}|_{w_i} \right \rangle$$
$$\omega_{i+1}=\omega_i+v_{i+1}$$

- `动量=权重衰减-学习率*梯度下降`
- 学习率随梯度变化变缓而减小

#### VGGNet
- [VGGNet](https://arxiv.org/abs/1409.1556v1)2015，K. Simonyan and A. Zisserman提出，是2014 年ILSVRC竞赛分类任务的第二名（第一名是GoogLeNet）和定位任务的第一名。
- 泛化能力强，适合迁移学习

##### 网络改进
- 对卷积核和池化大小进行了统一： 3×3卷积和2×2最大池化操作
- 采用卷积层堆叠的策略，将多个连续的卷积层构成卷积层组
- 采用小的卷积滤波器，网络更深（16/19层）
- 不采用LRN(Local Response Normalization)

##### 网络结构
- VGG-16有16个卷积层或全连接层，包括五组卷积层和3个全连接层，即： 16=2+2+3+3+3+3。

![VGG网络结构](/images/pasted-123.png)

![VGG网络结构](/images/pasted-124.png)

- 网络包含5组卷积操作，每组内的卷积层具有相同的结构，每组包含1~3个连续的卷积层，每两个卷积层之间为ReLU层。
- VGG的卷积层，特征图的空间分辨率单调递减，特征图的通道数单调递增。使得输入图像在维度上流畅地转换到分类向量。AlexNet的通道数无此规律，VGGNet后续的GoogLeNet和ResNet均遵循此维度变化的规律。

![卷积层分辨率递减，通道数递增](/images/pasted-126.png)

##### 卷积堆叠
- 与单个卷积层相比，卷积堆叠可以增加感受野，增强网络的学习能力和特征表达能力
- 与具有较大核的卷积层相比，采用多个具有小卷积核的卷积层串联的方式能够减少网络参数
- 在每层卷积之后进行ReLU非线性操作可提升网络的特征学习能力

![卷积堆叠](/images/pasted-125.png)

##### 训练
- 训练是通过使用带有动量的小批次梯度下降的反向传播来优化多类别逻辑回归目标
- 批次大小设置为256，动量为0.9。 通过权重衰减正则化（L2惩罚因子设置为5 × 10−4）和前两个全连接层的丢弃（丢弃率设置为0.5）对训练进行正则化。 
- 学习率初始设定为10−2， 然后当验证集准确率度停止改善时，学习率降低10倍

- 单尺度训练：训练图像大小裁剪为固定大小`224*224`(s=224)或`384*384`(s=384)
- 多尺度训练：尺度抖动，训练图像大小随机在`s∈[256,512]`的范围中

- 训练时，将不同尺寸的s，随机裁剪为`224*224`的训练样本
- 测试时，将图像缩放到固定尺寸Q=s或0.5(256+512)，对网络最后的卷积层使用滑动窗口进行分类预测，对不同的窗口分类结果取平均。

##### 多剪裁
- Multi-Crop evaluation: 从测试图像中剪切出一大批`224*224`的小块图像，在输出时取平均得到一个1000维的平均结果。

##### 密集评估
- Dense evaluation: 测试图像的尺寸不作任何变化，将最后一个卷积层和全连接层看成为一个卷积操作，然后对所作的结果进行平均。 全卷积网络因为没有全连接的限制，因而可以接收任意宽或高的输入
- 首先将全连接的层转换为卷积层（第一个FC层转换到7×7卷积层，最后两个FC层转换到1×1转换层）。 然后将得到的全卷积网络应用于整个（未剪切的）图像。
- 比如，对于训练图像其大小为`224*224`，经过最后一个卷积得到7×7×m的特征图，然后输出为1000个结点。这是训练好的网络。
- 对于一个`448*448`的测试图像，经过训练好的网络的映射，最后会输出`2*2*1000`大小的结果，将`2*2`里面的内容进行平均，即得到1000维向量

#### NiN
- [NiN](https://arxiv.org/abs/1312.4400)，2014，新加坡国立大学颜水成教授提出的，其中思想后续被普遍应用。他的思路是，用微小的神经网络代替卷积核，达到提升效率的目的。

##### Mlpconv
- 在网络中构建微型网络Mlpconv，它对conv特征进行了组合，提高了卷积的有效性
- 对单个像素， 1x1卷积等效于该像素点在所有特征上进行一次全连接计算
- Mlpconv中的全连接可以通过1×1卷积快速实现

![NiN](/images/pasted-127.png)

###### Mlpconv的作用
  1. 通过叠加更多的卷积结果，经过ReLU激励，能在相同的感受野范围中提取更强的非线性特征。即上层特征可以更复杂的映射到下层。
  2. 使用1×1卷积进行降维，降低了计算复杂度，并且保持图像大小

###### 计算策略
- 传统卷积+relu可表达为：$f_{i,j,k}=max(w_{k}^{T}x_{i,j},0)$，ij为图块中心店的位置，k为特征图通道索引
- Mlpconv：$f_{i,j,k}^l=max({w_{k}^{l}}^Tx_{i,j}+b_{k_j},0)$，l为通道索引，`Mlpconv = convolution + 1×1 convolution +…+ 1×1 convolution`

##### 全局平局池化
- 用全局平均值池化代替全连接层，减小了参数数量，降低了网络复杂度和过拟合

![全局平局池化](/images/pasted-128.png)

- 改变原先展平操作，而直接求平均

##### 网络结构
- 在论文中， NIN网络模型堆叠了三个mlpconv层和一个全局平均池化层

#### GoogLeNet
- [GoogLeNet](https://arxiv.org/abs/1409.4842)，2015，由google开发人员开发，在ILSVRC2014竞赛图像分类任务第一名。

##### 改进
- 深的网络：22层（包括池化层共27层）
- 引入了高效的 “Inception” 模块：设计良好的局部网络拓扑，然后将这些模块堆叠在一起。
- 无全连接层，使用平均池化代替，没有参数，不会过拟合。
- 参数数量仅为AlexNet的1/12

![Inception module](/images/pasted-129.png)

##### Naive Inception module
- 朴素Inception module

![Naive Inception module](/images/pasted-130.png)

![Naive Inception module](/images/pasted-131.png)

- 对前一层的输入进行并行滤波操作：
- 多个感受野大小（1x1,3x3,5x5)的卷积
- 池化操作（3x3）
- 所有滤波器输出沿深度（depth-wise）拼接在一起

**设计理念**
- 图像中的突出部分可能具有极大的尺寸变化，信息位置的这种巨大变化，卷积操作选择正确的核大小比较困难。
- 对于较全局分布的信息，首选较大的核，对于较局部分布的信息，首选较小的核。
- 非常深的网络容易过拟合。它也很难通过整个网络传递梯度更新。
- 简单地堆叠大卷积运算导致计算复杂度较高。

- Naive Inception缺点：高计算复杂度

> Conv Ops|-
> -|-
> [1x1 conv, 128] |28x28x128x1x1x256
> [3x3 conv, 192] |28x28x192x3x3x256
> [5x5 conv, 96] |28x28x96x5x5x256
> Total: |854M ops

- 解决方案: 引入“bottleneck”瓶颈层，其使用 1x1 卷积来减小特征深度

##### Inception module with dimension reduction
- 具有降维的Inception Module

![降维的Inception Module](/images/pasted-132.png)

1. Shortcut连接： 1×1卷积
2. 多尺度滤波1： 1×1卷积+3×3卷积 （不同感受野结合）
3. 多尺度滤波2： 1×1卷积+5×5卷积 （不同感受野结合）
4. 池化分支 ： 3×3 pooling + 1×1卷积

- inception结构的主要贡献：一是使用1x1的卷积来进行降维；二是在多个尺寸上同时进行卷积再聚合

- 计算复杂度
> Conv Ops|-
> -|-
> [1x1 conv, 64] |28x28x64x1x1x256
> [1x1 conv, 64] |28x28x64x1x1x256
> [1x1 conv, 128] |28x28x128x1x1x256
> [3x3 conv, 192] |28x28x192x3x3x64
> [5x5 conv, 96] |28x28x96x5x5x64
> [1x1 conv, 64] |28x28x64x1x1x256
> Total: |358M ops (<< 854M ops naive)

##### 多尺度聚合的作用
- 多尺度：对输入特征图分别在3×3和5×5的卷积核上进行滤波，提高了所学特征的多样性，增强了网络对不同尺度的鲁棒性。
- 多层次：符合Hebbian原理（“fire togethter, wire together” ），即通过1×1卷积把具有高度相关性的不同通道的滤波结果进行组合汇聚一起，起到加速收敛的作用。

##### GooLeNet网络结构

![GooLeNet](/images/pasted-133.png)

![GooLeNet](/images/pasted-134.png)

- 传统卷积操作：Conv-Pool-2x Conv-Pool
- 9个堆叠的Stacked Inception Modules
- 分类输出：Classifier output(removed expensive FC layers!)，使用了最大平均池化，代替了全连接层，没有参数，不会产生过拟合问题
- 辅助分类器：辅助分类输出，用于在较低层注入额外的梯度(AvgPool-1x1Conv-FC-FC-Softmax)，缓解梯度消失问题

##### 损失函数
- 训练时的Inception net的总损失 ：$total_{loss} = real_{loss} + 0.3 × aux_{loss_1} + 0.3×aux_{loss_2}$
- 2个附加的辅助损失纯粹用于训练目的，在推理（测试）过程中被忽略。


#### BN-Inception
- [BN-Inception](https://arxiv.org/abs/1502.03167)，2015，google提出对GoogLeNet Inception v1的改进

##### 改进
- 引入Batch Normalization（批归一化）：目前BN已经成为几乎所有卷积神经网络的标配技巧
- 5x5卷积核 -> 2个串联的3x3卷积核

##### Batch Normalization
- 内部协变量偏移（Internal Covariate Shift）:训练时网络参数的变化引起的网络激活分布的变化
- 小的扰动会引起后续大的变化，网络训练变得复杂。

![引入BN的原因](/images/pasted-135.png)

![内部协变量偏移](/images/pasted-136.png)

- 2018年的论文认为BN可以优化的原因，不是协变量偏移，而是BN使得优化地貌更平滑，使梯度更具预测性和稳定性，达到更快训练的目的。

![BN优化的原因分析-VGG网络](/images/pasted-137.png)

- BN调整的位置：卷积 -> BN -> ReLU
- 每一维度变为标准高斯分布：$\hat{x_i}\rightarrow \frac{x_i-\mu_B}{\sqrt{\sigma^2_B+\epsilon}}$，ϵ防止分母为0。这样全部数据都为标准分布，又不能表达输入数据的真实分布
- 为了能使工作在激活的非线性区，再进行尺度缩放和平移：$y_i\rightarrow\gamma\hat{x_i}+\beta\equiv BN_{\gamma,\beta}(x_i)$，利用γ(scale)和β(shift)做线型变换，做一个逆转操作。$B={x_1...x_m}$是mini-batch的样本空间，γ和β是习得的，利用小批量得到新的均值和方差替换整个训练集的均值和方差

- 预测时，均值方差不应该取决于随机小批量的数据，常用的方法是通过移动平均估算整个训练集的样本均值和方差，得到一个确定输出。

#### Inception-V2 V3
- [Inception-v2](https://arxiv.org/abs/1512.00567)


##### 改进
- Inception-V2 V3优化了Inception Module的结构，有三种
  - 5x5卷积由两个3x3替代，整个模块的处理后的特征图大小：35x35grid
  - 3x3由1x3和3x1替代，整个模块经过1xn的替代后变为：17x17grid
  - 这种模块更宽，特征图大小：8x8grid

![5x5卷积由两个3x3替代](/images/pasted-139.png)
![3x3由1x3和3x1替代](/images/pasted-140.png)
![inception module](/images/pasted-141.png)

- 辅助分类器的作用进行了研究
  - 辅助分类器在训练过程即将结束、准确度接近饱和时才会有很大贡献， 并不会帮助更快收敛。
  - 它们起到正则化作用，特别是具有BatchNorm或Dropout操作时。
  - 两个辅助分类器中较低层的那个可以去掉

- 引入了标签平滑正则化(label-smoothing regularization, or LSR)，可以避免过拟合，防止网络对于某一个类别预测过于自信。
  - 软标签定义：$q'(k|x)=(1-\epsilon)\delta_{k,y}+\epsilon u(k)$ $q'(k)=(1-\epsilon)\delta_{k,y}+\frac{\epsilon}{K}$，`ϵ`是极小数，这样ground truth对应标签有大部分的概率，而其他类别也有一小部分概率。
  - 新的交叉熵损失变为：$H(q',p)=-\sum_{K}^{k=1}{\log{p(k)q'(k)}}=(1-\epsilon)H(p,q)+\epsilonH(u,p)$，类别K=1000，u(k)=1/1000，ϵ=0.1


##### 网络配置
- v2中有10个inception的堆叠,3个35x35的module，5个17x17的module，2个8x8的module
![v2的网络配置](/images/pasted-142.png)

- v3的结构就是在v2的基础上，加上标签平滑和辅助分类器
![v3的网络配置](/images/pasted-143.png)

#### ResNet
- [ResNet](https://arxiv.org/abs/1512.03385)2016，在多项比赛中取得了第一，并且指标远远超过第二

##### 改进
- 更深的网络：152层
- 残差映射

##### 残差映射
- 作者发现简单堆叠更深的CNN网络并不会提升学习能力，反而减弱了模型学习能力。

![CNN网络堆叠](/images/pasted-144.png)

- 解决方法：使用网络层来学习残差映射(residual mapping )而不是直接学习期望映射

![残差映射](/images/pasted-145.png)

- ResNet增加了“短路”连接(shortcut connection)或称为跳跃连接(skip connection)，残差的引入去掉了主体部分，从而突出了微小的变化，模型只需要学习微小变化即可，使得网络更容易学习。

**不同网络结构直连**
- 当𝐹和𝑥相同维度时，直接相加(element-wise addition)，$y=F(x,w_i)+x$
- 当𝐹和𝑥维度不同时，需要先将𝑥做一个变换(linear projection)，然后再相加，$f(x,w_i)+w_sx$，𝑊𝑠仅仅用于维度匹配上。
- 对于𝑥的维度变换，一种是zero-padding，另一种是通过1x1的卷积。

##### 网络结构
**基准网络**
- 基于VGG19的架构，首先把网络增加到34层，增加过后的网络叫做plain network，在此基础上，增加残差模块，得到Residual Network

![Residual Network](/images/pasted-146.png)

- 把网络增加到34层， 采用两个设计原则， 1）对于有相同的输出特征图尺寸，滤波器的个数相同； 2）当特征图尺寸减半时，滤波器的数量加倍。
- 下采样的策略是直接用stride=2的卷积核。网络最后末尾是一个全局卷积平均池化层和一个1000的全连接层（后面接softmax）。

**残差网络**
- 在基准网络的基础上，插入了shortcut connections。
- 当输入输出具有相同尺寸时， identity shortcuts可以直接使用（实线部分），就是公式1；
- 当维度增加时（虚线部分），有以下两种选择：
  - A:仍然采用恒等映射，超出部分的维度使用0填充；
  - B:利用1x1卷积核来匹配维度。 
- 对于上面两种方案，当shortcuts通过两种不同大小的特征图时，采取A或B方案的同时， stride=2。

##### 残差模块
- 两种残差模块
  - 两个3x3的卷积核
  - 输入是256维，两个1x1的卷积核，通道数不一样，一个3x3卷积核。也叫做瓶颈残差模块

![两种残差模块](/images/pasted-147.png)

- 在训练浅层网络的时候，选用前面这种
- 而如果网络较深(大于50层)时，会考虑使用后面这种(bottleneck)来提高效率。网络的参数减少了很多，训练也就相对容易一些

##### 对残差模块提高效率的研究
- 残差模块平滑了损失地貌

![残差模块提高收敛](/images/pasted-148.png)

- 有残差的模块等效于多个网络并联

![残差模块等效于多个网络并联](/images/pasted-149.png)

#### Inception-V4 ResNet
- [Inception-V4](https://arxiv.org/abs/1602.07261)

##### 改进
**Inception-v4**
- 更深、更宽、更规整的网络架构

**Inception-ResNet**
- 改进的Inception模块和残差连接的结合

##### Inception-v4网络结构

![Inception-v4网络结构](/images/pasted-150.png)

- stem层包括：卷积、池化、聚合，没有标记“V” 的卷积使用same的填充原则，
即其输出网格与输入的尺寸正好匹配。标记“V” 的卷积输出激活图（output activation map）的网格尺寸也相应会减少。
![stem of Inception-v4 and Inception-ResNet-v2](/images/pasted-151.png)

- Inception-v4的ABC block也进行了一些更改

![grid module](/images/pasted-153.png)

- 不同Inception连接时，使用了Reduction模块，减少了feature map，却增加了filter bank

![Reduction模块](/images/pasted-152.png)

##### Inception-ResNet网络结构
- 引入直连，与ResNet结合，让网络又宽又深

![Inception-ResNet网络结构](/images/pasted-154.png)

![stem of Inception-ResNet-v1](/images/pasted-155.png)

![Inception-ResNet-v1 grid Modules](/images/pasted-157.png)

![Inception-ResNet-v2 grid Modules](/images/pasted-159.png)

![Reduction-B module](/images/pasted-160.png)


#### ResNeXt
- [ResNeXt](https://arxiv.org/abs/1611.05431)2017，Inception借鉴ResNet提出Inception-ResNet，ResNet也借鉴Inception提出ResNeXt，主要就是单路卷积变成多个支路的多路卷积，进行分组卷积。

###### Inception改进
- Inception卷积范式：将多个不同的卷积视野融合，缺点就是太复杂，人工设计痕迹过重

![Inception卷积范式](/images/pasted-161.png)

- 分析了神经网络的标准范式，就是先对输入的m个元素，分配到m个分支(split)，进行权重加权(transform)，然后求和(merge)，最后经过一个激活：$\sum_{i=1}^{D}{w_ix_i}$。

![神经网络的标准范式](/images/pasted-162.png)

- 由此归纳出神经网络的一个通用的单元可以用如下公式表示：$F(x)=\sum_{i=1}^{C}{T_i(x)}$
- 聚合变换：变换𝑇可以是任意形式。类似于一个简单的神经元， 𝑇𝑖将𝑥投射到一个低维嵌入中，然后对其进行转换。
- 一共有C个独立的变换，作者将C称之为Cadinality（基数）,并且指出，基数C对于结果的影响比宽度和深度更加重要。

- Network-in-Neuron: expands along a new dimension扩展新的维度
- Network-in-Network (NIN): increases the dimension of depth增加或者减少深度方向的维度

##### 网络结构
- 与ResNet相比，模块更加复杂，增加了Cadinality，通过多个并行路径(“cardinality” ) 增加残差块的宽度。并行路径在精神上与Inception模块类似
![ResNet与ResNeXt的基本构建块](/images/pasted-163.png)

#### DenseNet
- [DenseNet](https://arxiv.org/pdf/1608.06993.pdf)2017，借鉴了ResNet的思想

##### 改进

![DenseNet](/images/pasted-164.png)

- 在dense block中，每一层以前馈方式连接到每一个其它的层
- 减轻梯度消失，加强特征传播，鼓励特征重用
- 每一层均直接与前面的所有层相连，实现特征的重复利用；同时把网络的每一层设计得特别“窄”，即只学习非常少的特征图，达到减少参数的作用。

- 传统卷积网路的前向公式：$x_l=H_l(x_{l-1})$
- ResNet网路的前向公式：$x_l=H_l(x_{l-1})+x_{l-1}$
- DenseNet网路的前向公式：$x_l=H_l([x_0,...x_{l-1}])$

##### 网络结构
**DenseNet-A**
- 输入经过BN、ReLU、Conv，变为k通道的输出
![DenseNet-A](/images/pasted-165.png)

**DenseNet-B**
- 引入了1x1的瓶颈层，减少输入特征图的数量，提高计算效率
![DenseNet-B](/images/pasted-166.png)

##### 增长率growth rate
- 每个函数𝐻产生𝑘个特征图。DenseNet可以具有非常窄的层，例如， 𝑘 = 12。将超参数𝑘称为网络的增长率(growth rate)。
- 一种解释是，每个层都可以访问其块中的所有前面的特征图，因此可以访问网络的“集体知识”（collective knowledge）。 可以将特征图视为网络的全局状态。 每个层都将自己的𝑘个特征图添加到此状态。
- 增长率规定了每层为全局状态贡献了做出多少新信息。 一旦写入，全局状态可以从网络中的任何地方访问，并且与传统网络体系结构不同，不需要在层与层之间复制它。

![denseNet with 3 dense blocks](/images/pasted-167.png)

- 两个相邻块之间的层称为过渡层(transition layers)，通过卷积和池化来改变特征图大小。为了进一步提高模型的紧凑性，可以减少过渡层的特征图数量。 如果密集块包含𝑚个特征图，让下面的过渡层生成θm个输出特征图，其中0<𝜃≤1被称为压缩因子

<!-- > 模型轻量化 -->
<!-- #### SqueezeNet -->
<!-- #### Xception -->
<!-- #### MobileNet -->
<!-- #### MobileNet V2 -->
<!-- #### ShuffleNet -->
<!-- #### ShuffleNet V2 -->

### 目标检测
#### 数据集

名称|说明
-|-
[PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/)|PASCAL VOC竞赛提供的数据集，包含了20类的物体，主要任务是分类，检测，分割，人体布局，人体动作识别
[ImageNet](http://www.image-net.org/)|计算机视觉数据集，包含14,197,122个张图片， 21,841个Synset索引。它一直是评估图像分类算法性能的基准。
[COCO](http://cocodataset.org)|

> 基于建议框的方法：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN、FPN
> 免建议框方法YOLO、SSD、DSSD、RetinaNet

#### R-CNN
[R-CNN](https://arxiv.org/abs/1311.2524v3)2014，是使用CNN目标检测的开山之作。

##### 主要流程
输入图像->获取候选区域推荐列表，并将区域大小统一变换->卷积神经网络训练->区域内目标分类

**区域建议**
- 使用Selective Search方法选择约2000个置信度最高的区域建议


**区域变换**

- ![区域变换](/images/pasted-168.png)

  - A:原始区域建议
  - B:有背景
  - C:没有背景
  - D:变换固定大小

**提取特征**
- R-CNN中的CNN使用AlexNet， softmax层改成(N+1)-way，其余不变
- 由于CNN的参数量巨大，训练CNN需要大量的样本。

**分类训练**
- 使用SVM分类器训练，采用hard negative mining方法提升SVM分类器的准确度


##### 区域建议的方法
- 滑动窗口：窗口大小进行穷举扫描整图
- 选择性搜索(selective search)：自底向上，像素级别的提取，相同则合并，逐步扩大区域，合并到不同尺度


##### Hard Negative Mining (困难负样本挖掘)

- SVM训练完成后，如果完全分类正确，所有正样本的输出概率都大于0.5，而所有负样本的输出概率都小于0.3
- 但常见的情况是有一部分的负样本的输出概率也大于0.5的, 这些样本就称之为“False Positives”
- 如果把 产生“False Positives” 的样本收集起来，对SVM进行二次训练，一般可提升SVM的分类准确度



#### YOLO-v1
#### YOLO-v2
#### YOLO-v3
#### YOLO-v4
#### SSD
#### DSSD
#### RetinaNet


### 语义分割
#### FCN
#### DeconvNet
#### SegNet
#### U-Net
#### DeepLab-v1
#### DeepLab-v2
#### DeepLab-v3
#### RefineNet
#### PSPNet
#### GCN

### 实例分割
#### Mask R-CNN

### 全景分割
#### Panopatic FPN

### 人脸识别
#### DeepFace
#### DeepID
#### DeepID2
#### DeepID2+
#### DeepID3
#### FaceNet
#### MTCNN
#### VGGFace
#### SphereFace
#### CosFace
#### ArcFace

### 图像描述
#### RNN
#### LSTM
#### GRU

### 图像检索
#### 最近邻检索
#### 局部敏感哈希(LSH)算法
#### DH
#### SDH
#### CNNH
#### NINH
#### DSRH
#### DRSCH
#### DLBHC

### 图像生成
#### GAN
#### CGAN
#### DCGAN
#### WGAN
#### WGAN-GP
#### CycleGAN
#### SRGAN





# 循环神经网络RNN
- RNN (Recurrent Neural Network)，可用于图像描述(一对多)，情感分析(多对一)，机器翻译(多对多)，视频逐帧分类(多对多)
- 循环神经网络具有“环”，允许信息持续存在

![循环神经网络按时间顺序展开](/images/pasted-110.png)

- $h_t=f_w(h_{t-1},x_t)$

- $h_t$:new state
- $fw$:function parameterized by w，RNN更新选择使用tanh函数
- $h_{t-1}$:old state
- $x_t$:input vector

- 输出：$\hat{y}_t=w_{hy}h_t$
- 隐藏层状态：：$h_t=tanh(w_{hh}h_{t-1}+w_{xh}x_t)$

- 梯度爆炸：反向传播计算h0时，会多次进行Whh和f'，矩阵奇异值比较大就会梯度爆炸，解决方法就是梯度裁剪
- 一种方法就是小批量的裁剪
- 另一种方法就是裁剪一个摩norm||g||：$if\;||g||>v\;g\rightarrow\frac{gv}{||g||}$

- 梯度消失：奇异值小于1时，多次相乘则会产生梯度消失。

## RNN
- 在标准RNN中，该重复模块具有非常简单的结构，例如单个tanh层。

![RNN网络结构](/images/pasted-111.png)

- 隐藏层状态：：$h_t=tanh(w_{hh}h_{t-1}+w_{xh}x_t)$

## lstm
- 使用门控制单元，跟踪信息，对长期信息的依赖问题能够很好解决

![lstm网络结构](/images/pasted-112.png)

- LSTM的关键是单元状态(cell state)， 水平线贯穿图的顶部。单元状态有点像传送带。 它直接沿着整个链运行， 只有一些小的线性相互作用。信息很容易沿着它不变地流动。这个结构对梯度消失有很好的抑制。
- LSTM能够移除或添加信息到单元状态， 由称为门的结构调节。门是一种可选择通过信息的方式。 它们由Sigmoid神经网络层和逐点乘法运算组成。sigmoid层输出0到1之间的数字， 描述每个组件应该通过多少。 值为零意味着“不让任何东西通过” ， 而值为1则意味着“让一切都通过”

- 遗忘门：$f_t=\sigma(w_f\cdot[h_{t-1},x_t]+b_f)$，遗忘门查看ℎ𝑡−1和𝑥𝑡 ，并为𝐶𝑡−1输出0到1之间的数字。 1代表“完全保留”，而0代表“完全遗忘”。

![遗忘门](/images/pasted-113.png)

- 输入门：确定在单元状态中存储哪些新信息$i_t=\sigma(w_i\cdot[h_{t-1},x_t]+b_i)$ $\hat{C}_t=tanh(W_h\cdot[h_{t-1},x_t]+b_c)$，更新旧的单元状态C_{t-1}到新的单元状态C_t$C_t=f_t*C_{t-1}+i_t*\hat{C}_t$，将旧状态乘以𝑓𝑡，忘记之前决定遗忘的事情然后加上缩放后的候选值$i_t*\hat{C}_t$

![输入门](/images/pasted-114.png)

![输入门](/images/pasted-115.png)

- 输出门：要输出的内容$o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)$ $h_t=o_t*tanh(C_t)$，输出将基于单元状态(cell state)，但将是滤波后的版本


![输出门](/images/pasted-116.png)


## GRU
- GRU模型是LSTM模型的简化版，仅包含两个门函数（reset gate和update gate）。reset gate决定先前的信息如何结合当前的输入， update gate决定保留多少先前的信息。如果将reset全部设置为1，并且update gate设置为0，则模型退化为RNN模型。


![GRU网络结构](/images/pasted-117.png)

- $z_t=\sigma(W_z\cdot[h_{t-1},x_t])$
- $r_t=\sigma(W_r\cdot[h_{t-1},x_t])$
- $\hat{h}_t=tanh(W\cdot[r_{t}*h_{t-1},x_t])$
- $h_t=(1-z_t)*h_{t-1}+z_t*\hat{h}_t$

# 生成对抗网络GAN
- GAN (Generative Adversarial Network)