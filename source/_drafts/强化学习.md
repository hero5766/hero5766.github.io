---
title: 强化学习
author: hero576
tags:
  - algorithm
categories:
  - AI
date: 2020-09-10 15:57:46
---
<!-- more -->

# 简介
## 介绍
- 游戏测试平台：
  - Atari2600
  - ALE: (Arcade Learning Environment)
  - OpenAI: https://gym.openai.com/docs/


## 价值函数估计 Value Function Approximation
- 在Q-Learning学习过程中，用Q(S,A)表示value function。有限个状态state和行动action的情况下，维护一个Q-Table，不断更新，直至收敛。
- 对于复杂情况，state数量可能是无穷大。解决方法，可以将连续变量离散化，使得问题退回到可以用Q-Table解决。第二种是使用f(S,A,θ)来估计Q(S,A)，其中θ是科学参数。
- 学习θ方法包括：
  - 线性函数直接用最小二乘法
  - 定义损失函数使用梯度下降算法优化
  - 类似进化的方法学习

# Deep Q-Learning
- Playing Atari with Deep Reinforcement Learning
## 模型总览
- 模型为CNN结构，定义网路输出所有action的Q value
![](/images/pasted-397.png)


## 输入信息预处理
- 图像：
  - gray scale
  - down sampling (210x160-->110x84)
  - center cropping(84x84)
- 游戏：
  - Key Frame：固定间隔选取frame，让模型看不见中间过程；
  - 4个Key Frame作为Q-Network的输入
  - Reward Clipping: 将所有的reward简化为±1和0


## 回放机制 Replay Buffer
- 原理：
  - 存储过去遇到的transition
  - 忘记太久远的transition
  - 训练时，从Replay Buffer中随机抽样进行训练


## Semi-Gradient Methed
- 梯度：$(y_j-Q(\phi_j,a_j,\theta))^2$
- 其中：$y_j=r+\gamma\max_{a'}Q(s',a';\theta_{t-1})$
- Q函数不和之前的Q共享参数，使得当前状态得到的参数，使输出尽量贴近前面的状态，而不影响之前的参数。


# 代码
- gym的使用
```py
import gym, random
import numpy as np
import matplotlib.pyplot as plt
# 激活环境
env1 = gym.make('LunarLander-v2')
env2 = gym.make('Pong-v0')
# 基本信息
print(env1.obseration_space) # state
print(env1.obseration_space.shape)
print(env1.action_space) # action 
print(env1.action_space.n)
# 初始化
state1 = env1.reset()
# 可视化
env = env1
new_state,reward,episode,info = env.step(action)
env.render()
# episode=True时，step有效，否则会报错
state = env1.reset()
done = False
action = 3 # 固定
action = env.action_space.sample() # 随机action
for i in range(1000):
  env1.render()
  state,reward,done,_ = env1.step(action)
  if done:
    break
# 关闭
env1.close()
env2.close()
```




