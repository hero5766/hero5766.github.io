---
title: 长尾数据处理
author: hero576
tags:
  - robotic
categories:
  - programme
date: 2020-09-08 15:20:52
---

> 长尾数据处理

<!-- more -->

# 过采样和重采样
- 实际效果并不是太好

# 每个Batch对每类样本设置比例，保证在一个Batch里是相对均衡的
- 大样本不能充分训练，导致准确率下降，同时小样本也容易过拟合

# focal loss
- 通过减少易分类样本的权重，从而使得模型在训练时更专注于难分类的样本
- 假设一个二分类，样本x1属于类别1的pt=0.9，样本x2属于类别1的pt=0.6，显然前者更可能是类别1，假设γ=1，那么对于pt=0.9，调制系数则为0.1；对于pt=0.6，调制系数则为0.4，这个调制系数就是这个样本对loss的贡献程度，也就是权重，所以难分的样本（pt=0.6）的权重更大。

$$FL(p_t)=-\alpha_t(1-p_t)^\gamma\log(p_t)$$

- 一般而言当γ增加的时候，a需要减小一点（实验中γ=2，a=0.25的效果最好）

- 但是这里有坑一定要多注意
  1. focal loss多用于检测，在正样本和负样本比例1:1000的情况下比较有效，对于一般的样本不均衡其实效果不大，对于 easy sample dominant有作用，这两个问题并不等价。
  2. 用这个loss预测出得概率都会比较低一点。若γ过大时，可以理解其实对小样本过于over fit，这样效果也不好，所以这是为什么在机器学习里，对于小样本尽可能使用插值法构建新样本，也不会用小样本反复训练。

# loss reweight
- [loss reweight](https://arxiv.org/pdf/1901.05555.pdf)
- 如果对loss不加约束的话，大样本数据占主导，影响小样本，如果加约束的话，则会大大压缩大样本数据的训练，所以要找到一个中和的约束方法。这个模型想要可以区别major class和minor class，他的操作是在loss里面添加了一个和有效样本数量成反比的类平衡加权项。论文有两点贡献：
  1. 提供一种可以学习有效样本数量的方案来设计模型
  1. 提供了如何在sigmoid CE loss，softmax CE loss，focal loss上进行修改，并且通过实验验证提升
- 他引申出每个类都存在一个容量为N的特征空间S，那么每一群数量为n的样例都存在一个有效样本数量:$En=\frac{1-\beta^n}{1-\beta}$，$\beta=\frac{N-1}{N}$

推导|结论
-|-
`n=1`|$E_1=\frac{1-\beta}{1-\beta}=1$
`n=N-1`|抽到以抽样样本概率$P=\frac{E_{n-1}}{N}$，$E_n=PE_{n-1}+(1-P)(E_{n-1}+1)=1+\frac{N-1}{N}E_{n-1}$，设$E_{n-1}=\frac{1-\beta{n-1}}{1-\beta}$，$E_n=E_{n-1}+(1-p)$，同时$En=\frac{1-\beta^n}{1-\beta}$，所以当样本为无穷大时，每一类容量为：$N=\lim_{n\rightarrow\infty}\sum_{j=1}^{N}\beta^{j-1}=\frac{1}{1-\beta}$
推论|$\lim_{\beta\rightarrow 1}E_n=N$

- En的渐近性质表明，当N较大时，有效样本数与样本数N相同。在这种情况下，我们认为唯一原型数N较大，因此没有数据重叠，每个样本都是唯一的。在另一个极端，如果N = 1，这意味着我们相信存在一个单一的原型，所以这个类中的所有数据都可以通过这个原型通过数据扩充、转换等来表示。

- 现假设label$y\in[1,2,\dots,C]$，C是总类别数，模型估计每一类的概率为：$p=[p_1,p_2,\dots,p_C]^T$，损失函数：$L(p,y)$，每一类的有效采样数量：$E_{n_i}=\frac{1-\beta_{i}^{ni}}{1-\beta_i}$，$\beta_i=\frac{N_i-1}{N}$，$N_i=N$，$\beta_i=\beta=\frac{N-1}{N}$
- 我们可以用$\frac{1}{E_n}$来作为loss的权重，以CEloss为例：
$$CB(p,y)=\frac{1}{E_{n_y}}L(p,y)=\frac{1-\beta}{1-\beta^{n_y}}L(p,y)$$

- $n_y$是每一类的采样数量，图3说明了不同的$\beta,n_y$下，$\frac{1}{E_n}$的输出

![输出](/images/pasted-266.png)

- β = 0：no re-weighting
- β → 1：inverse class frequency

- 下面是作者改造的公式

$$CE_{softmax}(z,y)=-log(\frac{exp(z_y)}{\sum_{j=1}^{C}exp(z_j)})$$
$$CB_{softmax}(z,y)=-\frac{1-\beta}{1-\beta^{n_y}}log(\frac{exp(z_y)}{\sum_{j=1}^{C}exp(z_j)})$$
$$z_i^t=\begin{cases} z_i,&if\;i=y\\-z_i,&\;otherwise\end{cases}$$
$$CE_{sigmoid(z,y)}=-\sum_{i=1}^{C}log(sigmoid(z_i^t))$$
$$CB_{sigmoid(z,y)}=-\frac{1-\beta}{1-\beta6{n_y}}\sum_{i=1}^{C}log(sigmoid(z_i^t))$$
$$FL(z,y)=-\sum_{i=1}^{C}(1-p_i^t)^\gamma\log(p_i^t)$$
$$CB(z,y)=-\frac{1-\beta}{1-\beta^{n_y}}\sum_{i=1}^{C}(1-p_i^t)^\gamma\log(p_i^t)$$

![输出](/images/pasted-266.png)
![输出](/images/pasted-266.png)
- 可以看到，根据数据集不同，β是变化的。N可以解释为一个类原始样本的数量。与粗粒度数据集相比，细粒度数据集的N应该更小。例如，一个特定鸟类物种的原始样本的数量应该小于一个通用鸟类类的原始的数量。由于CIFAR-100中的类比CIFAR-10更细粒度，所以CIFAR100的N应该比CIFAR-10小。这就解释了β的影响。

- 计算权重的时候，实际是按照每个类别样本数量作为n值，得到每个权重后需要归一化然后乘以class number。

$$\sum_{i=1}^{C}\alpha_i=C$$

- [原文链接](https://blog.csdn.net/qq_28778507/article/details/93725573)