---
title: 机器学习的评价指标
author: hero576
tags:
  - algorithm
categories:
  - AI
date: 2020-09-06 19:44:00
---
> 评价指标
<!--more-->

# 简介
- 根据任务不同，检测性能指标也有区别[^1]
[^1]: [模型评价指标汇总](https://blog.csdn.net/m0_37313123/article/details/90407484)

任务|评价指标
-|-
回归分析|mse、rmse、mae、标准差、r2拟合优度
二分类|accuracy、precision、recall、F-score、PR curve、ROC-AUC curve、GINI系数
多分类|accuracy、宏平均、微平均、F-score
目标检测|IoU、AP、mAP

目标|评价指标
-|-
精度|precision、recall、F1-score、IoU、PR curve、AP、mAP
速度|前传耗时：传入图片到输出结果的时间，包括预处理，网络，后处理等<br>每秒帧数FPS：每秒处理图像数量<br>浮点运算量FLOPS：处理一张图片所需要的浮点运算数量，跟硬件无关，可以评估不同算法之间的检测速度

- 其他评价指标
  - 计算速度：分类器训练和预测需要的时间；
  - 鲁棒性：处理缺失值和异常值的能力；
  - 可扩展性：处理大数据集的能力；
  - 可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。


# 混淆矩阵[^2]
[^2]: [准确率和正确率的区别](https://www.cnblogs.com/mxp-neu/articles/5316989.html)、[神经网络性能评价指标汇总](https://zhuanlan.zhihu.com/p/73569538)

![混淆矩阵](/images/pasted-103.png)

指标|公式|说明
-|-|-
正准率accuracy|$$A=\frac{TP+TN}{P+N}$$|正负样本评估准不准
错误率error|$$E=\frac{FP+FN}{P+N}$$|正负样本评估准不准
准确率precision|$$P=\frac{TP}{P}$$|正样本评估准不准
召回率recall|$$R=\frac{TP}{TP+FN}$$|正样本评估是否全，召回率是覆盖面的度量，度量有多个正例被分为正例
F1|$$\frac{2}{F1}=\frac{1}{P}+\frac{1}{N}\\F1=\frac{2*P*R}{P+R}$$|结合准确率和查全率
含有度量参数β的F1|$$F_\beta=\frac{(1+\beta^2)PR}{\beta^2P+R}$$|有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数β来度量两者之间的关系。如果β>1, 召回率有更大影响，如果β<1,精确率有更大影响。自然，当β=1的时候，精确率和召回率影响力相同，和F1形式一样。含有度量参数β的F1我们记为Fβ,
特异性(specificity)|$$S=\frac{TN}{FP+TN}$$|所有真实负样本去掉FP部分后比上真实负样本，等于`1-FPR`
灵敏度(true positive rate ,TPR)|$$TPR=\frac{TP}{TP+FN}$$|所有实际正例中，正确识别的正例比例，它和召回率的表达式没有区别。
1-特异度(false positive rate, FPR)|$$FPR=\frac{FP}{FP+TN}$$|负正类率，实际负例中，错误得识别为正例的负例比例。

# PR
- 以精确率为y轴，以召回率为x轴，我们就得到了PR曲线。仍然从精确率和召回率的定义可以理解，精确率越高，召回率越高，我们的模型和算法就越高效。也就是画出来的PR曲线越靠近右上越好。

# ROC
- 以TPR为y轴，以FPR为x轴，我们就直接得到了RoC曲线。从FPR和TPR的定义可以理解，TPR越高，FPR越小，我们的模型和算法就越高效。也就是画出来的RoC曲线越靠近左上越好。

# AUC
- 从几何的角度讲，RoC曲线下方的面积越大越大，则模型越优。所以有时候我们用RoC曲线下的面积，即AUC（Area Under Curve）值来作为算法和模型好坏的标准。



# IoU
![IoU](/images/pasted-138.png)

- IoU(Intersection over Union)交并比，就是识别区域与实际区域，交集比上并集
$$IoU=\frac{Area of Overlap}{Area of Union}$$

![precision](/images/pasted-264.png)

- 准确率：
$$precision=\frac{Overlap}{detect box}$$
- kaggle比赛中的定义
$$precision=\frac{TP}{TP+FP+FN}$$
- kaggle比赛中单个图片多边界精度的定义
$$avg.precision=\frac{1}{n}\sum_{t=1}^{n}precision$$

- 召回率：
$$recall=\frac{Overlap}{object box}$$

# AP
- AP(average precision)：平均精度，模型在每个类别上的好坏
- mAP(mean AP)：所有样本的平均值，模型在所有类别上AP的平均值

- AP[0.5:0.05]：表示IoU的范围
- AP@.50：AP在IoU=0.5的结果
- AP small：area < 32 <sup>2</sup>px
- AP medium：32<sup>2</sup> < area < 96 <sup>2</sup>px
- AP large：area> 96 <sup>2</sup>px

- mAP：AP@[.5:.05:.95]，$mAP=\frac{mAP_{0.5}+mAP_{0.55}+\cdots+mAP_{0.95}}{10}$


## 例子
- 图像中一共5个苹果，判断结果置信度由大到小排序，判断了10次预测。
$$precision=\frac{TP}{TP+FP}$$ 
$$recall=\frac{TP}{TP+FN}$$

rank|whether correct|precision|recall
-|-|-|-
1|true|1.0|0.2
2|true|1.0|0.4
3|false|0.67|0.4
4|false|0.5|0.4
5|false|0.4|0.4
6|true|0.5|0.6
7|true|0.57|0.8
8|false|0.5|0.8
9|false|0.44|0.8
10|true|0.5|1.0


- 绘制曲线并做平滑，对某个点平滑时，取后面最大的值。

![精度和召回曲线](/images/pasted-183.png)


$$AP=\frac{1}{11}\times(AP_r(0)+...AP_r(1.0))$$

recall|max
-|-
0|1.0
0.1|1.0
0.2|1.0
0.3|1.0
0.4|1.0
0.5|0.57
0.6|0.57
0.7|0.57
0.8|0.57
0.9|0.5
1.0|0.5


# mAP
- mAP对测试所有图像的每一类AP求平均


# Dice Coefficient
- mask和prediction一般是sigmoid的输出，在(0,1)之间
$$Dice=\frac{2\cdot |mask\cap prediction|}{|mask|+|prediction|}$$

$$Dice Loss=-log(Dice)$$

- Dice和IoU(交并比)之间的关系

$$Dice=\frac{2|A\cap B|}{|A|+|B|}$$
$$IoU=\frac{A\cap B}{A\cup B}=\frac{|A\cap B|}{|A|+|B|-|A\cap B|}$$


